{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting with LSTMs -- Second round\n",
    "i.e., full dataset, transformed to PCA\n",
    "\n",
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 67\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "data = pandas.read_csv(\"../data/processed/train.csv\")\n",
    "notnull_data = data[data.notnull().all(axis=1)]\n",
    "train = notnull_data.values\n",
    "data2 = pandas.read_csv(\"../data/processed/test.csv\")\n",
    "notnull_data2 = data2[data2.notnull().all(axis=1)]\n",
    "test = notnull_data2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing dataset with PCA 100\n",
      "Total variance explained by %d components: 100\n",
      "0.630815671837\n"
     ]
    }
   ],
   "source": [
    "X_train = train[:,3:7558].astype(float)\n",
    "Y_trainA = train[:,7558] #Activity\n",
    "Y_trainS = train[:,7559] #Social\n",
    "X_test = test[:,3:7558].astype(float)\n",
    "Y_testA = test[:,7558]\n",
    "Y_testS = test[:,7558]\n",
    "\n",
    "# One hot encoding of the response variable (using dummy variables)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# encode class values as integers\n",
    "encoderA = LabelEncoder()\n",
    "encoderA.fit(Y_trainA)\n",
    "encoded_Y_trainA = encoderA.transform(Y_trainA)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_trainA = to_categorical(encoded_Y_trainA)\n",
    "encoderA.fit(Y_testA)\n",
    "encoded_Y_testA = encoderA.transform(Y_testA)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_testA = to_categorical(encoded_Y_testA)\n",
    "\n",
    "# encode class values as integers\n",
    "encoderS = LabelEncoder()\n",
    "encoderS.fit(Y_trainS)\n",
    "encoded_Y_trainS = encoderS.transform(Y_trainS)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_trainS = to_categorical(encoded_Y_trainS)\n",
    "encoderS.fit(Y_testS)\n",
    "encoded_Y_testS = encoderS.transform(Y_testS)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_testS = to_categorical(encoded_Y_testS)\n",
    "\n",
    "# We standardize on the basis of the training data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_st = scaler.transform(X_train)\n",
    "X_test_st = scaler.transform(X_test)\n",
    "\n",
    "# Number of components to extract from the dataset\n",
    "n_components = 100\n",
    "\n",
    "from sklearn import decomposition\n",
    "print 'Reducing dataset with PCA',n_components\n",
    "pca = decomposition.PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_st)\n",
    "X_test_pca = pca.transform(X_test_st)\n",
    "\n",
    "#print 'Variance explained:'\n",
    "#print pca.explained_variance_ratio_\n",
    "print 'Total variance explained by %d components:',n_components\n",
    "print sum(pca.explained_variance_ratio_)\n",
    "\n",
    "trainX = numpy.reshape(X_train_pca, (X_train_pca.shape[0], 1, X_train_pca.shape[1]))\n",
    "testX = numpy.reshape(X_test_pca, (X_test_pca.shape[0], 1, X_test_pca.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for training!\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# This is our winning architecture so far\n",
    "def create_LSTM3_PCA(n_outputs, batch_size = 1, trainShape1=100):\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    # stateful LSTM!\n",
    "    model.add(LSTM(200, batch_input_shape=(batch_size, 1, trainShape1), \n",
    "                   return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100, \n",
    "                   return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, \n",
    "                   return_sequences=False, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_LSTM2_PCA(n_outputs, batch_size = 1, trainShape1=100):\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    # stateful LSTM!\n",
    "    model.add(LSTM(300, batch_input_shape=(batch_size, 1, trainShape1), \n",
    "                   return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, \n",
    "                   return_sequences=False, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_LSTM1_PCA(n_outputs, batch_size = 1, trainShape1=100):\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    # stateful LSTM!\n",
    "    model.add(LSTM(400, batch_input_shape=(batch_size, 1, trainShape1), \n",
    "                   return_sequences=False, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, cohen_kappa_score\n",
    "\n",
    "def printValStats(model, testX, dummy_y_test, batch=1):\n",
    "    # Other performance/accuracy metrics\n",
    "    Y_pred = model.predict(testX, batch_size=batch)\n",
    "    model.reset_states()\n",
    "    print 'Performance of model on test set ----------------------------'\n",
    "    # Accuracy\n",
    "    print('Accuracy:')\n",
    "    print(accuracy_score(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1)))\n",
    "    # Kappa\n",
    "    print('Kappa:')\n",
    "    kappa = cohen_kappa_score(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1))\n",
    "    print(kappa)\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1))\n",
    "    numpy.set_printoptions(precision=2)\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    # AUC\n",
    "    roc = roc_auc_score(dummy_y_test, Y_pred, average='macro')\n",
    "    print('AUC score:')\n",
    "    print(roc)\n",
    "    return kappa, roc\n",
    "\n",
    "def plot_training(accs, val_accs, losss, val_losss, kappas, aucs):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(accs)\n",
    "    plt.plot(val_accs)\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(losss)\n",
    "    plt.plot(val_losss)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize kappa and auc\n",
    "    plt.plot(kappas)\n",
    "    plt.plot(aucs)\n",
    "    plt.title('Other performance')\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Kappa','AUC'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "import operator\n",
    "\n",
    "def get_max_values(list):\n",
    "    index, value = max(enumerate(list), key=operator.itemgetter(1))\n",
    "    return index, value\n",
    "\n",
    "print 'Ready for training!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Activity\n",
    "\n",
    "### 3-layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_36 (LSTM)                   (1, 1, 200)           240800      lstm_input_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)             (1, 1, 200)           0           lstm_36[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_37 (LSTM)                   (1, 1, 100)           120400      dropout_62[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)             (1, 1, 100)           0           lstm_37[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_38 (LSTM)                   (1, 50)               30200       dropout_63[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)             (1, 50)               0           lstm_38[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_35 (Dense)                 (1, 50)               2550        dropout_64[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)             (1, 50)               0           dense_35[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_36 (Dense)                 (1, 20)               1020        dropout_65[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)             (1, 20)               0           dense_36[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_37 (Dense)                 (1, 5)                105         dropout_66[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 395075\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "('Epoch', 1, '/', 20)\n",
      "Train on 4472 samples, validate on 1044 samples\n",
      "Epoch 1/1\n",
      "4471/4472 [============================>.] - ETA: 0s - loss: 1.3492 - acc: 0.4525Epoch 00000: val_acc improved from -inf to 0.46073, saving model to activity.weights--3lstmbis.best.hdf5\n",
      "4472/4472 [==============================] - 81s - loss: 1.3493 - acc: 0.4524 - val_loss: 1.3800 - val_acc: 0.4607\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.454022988506\n",
      "Kappa:\n",
      "0.263597787873\n",
      "Confusion matrix:\n",
      "[[192   8   0   0   0]\n",
      " [ 60 213  52   0   0]\n",
      " [148 120  69   0   0]\n",
      " [ 61   9  25   0   0]\n",
      " [ 14  59  14   0   0]]\n",
      "AUC score:\n",
      "0.663681903884\n",
      "('Epoch', 2, '/', 20)\n",
      "Train on 4472 samples, validate on 1044 samples\n",
      "Epoch 1/1\n",
      "4471/4472 [============================>.] - ETA: 0s - loss: 1.2898 - acc: 0.4742Epoch 00000: val_acc improved from 0.46073 to 0.49042, saving model to activity.weights--3lstmbis.best.hdf5\n",
      "4472/4472 [==============================] - 103s - loss: 1.2900 - acc: 0.4741 - val_loss: 1.3154 - val_acc: 0.4904\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.488505747126\n",
      "Kappa:\n",
      "0.308466048851\n",
      "Confusion matrix:\n",
      "[[195   2   3   0   0]\n",
      " [ 23 287  15   0   0]\n",
      " [140 169  28   0   0]\n",
      " [ 82  12   1   0   0]\n",
      " [  9  76   2   0   0]]\n",
      "AUC score:\n",
      "0.758234814164\n",
      "('Epoch', 3, '/', 20)\n",
      "Train on 4472 samples, validate on 1044 samples\n",
      "Epoch 1/1\n",
      "4471/4472 [============================>.] - ETA: 0s - loss: 1.2048 - acc: 0.5171Epoch 00000: val_acc improved from 0.49042 to 0.54023, saving model to activity.weights--3lstmbis.best.hdf5\n",
      "4472/4472 [==============================] - 124s - loss: 1.2047 - acc: 0.5172 - val_loss: 1.2861 - val_acc: 0.5402\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.545019157088\n",
      "Kappa:\n",
      "0.382142305373\n",
      "Confusion matrix:\n",
      "[[198   2   0   0   0]\n",
      " [ 21 278  26   0   0]\n",
      " [122 122  93   0   0]\n",
      " [ 77  12   6   0   0]\n",
      " [ 10  75   2   0   0]]\n",
      "AUC score:\n",
      "0.785372131709\n",
      "('Epoch', 4, '/', 20)\n",
      "Train on 4472 samples, validate on 1044 samples\n",
      "Epoch 1/1\n",
      "4471/4472 [============================>.] - ETA: 0s - loss: 1.1619 - acc: 0.5379Epoch 00000: val_acc improved from 0.54023 to 0.56609, saving model to activity.weights--3lstmbis.best.hdf5\n",
      "4472/4472 [==============================] - 139s - loss: 1.1618 - acc: 0.5380 - val_loss: 1.1501 - val_acc: 0.5661\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.57183908046\n",
      "Kappa:\n",
      "0.415673105096\n",
      "Confusion matrix:\n",
      "[[198   1   1   0   0]\n",
      " [ 21 208  96   0   0]\n",
      " [110  36 191   0   0]\n",
      " [ 78   9   8   0   0]\n",
      " [  7  64  16   0   0]]\n",
      "AUC score:\n",
      "0.803501171992\n",
      "('Epoch', 5, '/', 20)\n",
      "Train on 4472 samples, validate on 1044 samples\n",
      "Epoch 1/1\n",
      "4471/4472 [============================>.] - ETA: 0s - loss: 1.1069 - acc: 0.5762Epoch 00000: val_acc improved from 0.56609 to 0.59674, saving model to activity.weights--3lstmbis.best.hdf5\n",
      "4472/4472 [==============================] - 147s - loss: 1.1068 - acc: 0.5763 - val_loss: 1.1164 - val_acc: 0.5967\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.599616858238\n",
      "Kappa:\n",
      "0.443076357622\n",
      "Confusion matrix:\n",
      "[[183   0  17   0   0]\n",
      " [ 12 223  90   0   0]\n",
      " [ 54  63 220   0   0]\n",
      " [ 42   9  44   0   0]\n",
      " [  9  69   9   0   0]]\n",
      "AUC score:\n",
      "0.802025368517\n",
      "('Epoch', 6, '/', 20)\n",
      "Train on 4472 samples, validate on 1044 samples\n",
      "Epoch 1/1\n",
      "4471/4472 [============================>.] - ETA: 0s - loss: 1.0224 - acc: 0.6245Epoch 00000: val_acc did not improve\n",
      "4472/4472 [==============================] - 153s - loss: 1.0223 - acc: 0.6246 - val_loss: 1.2869 - val_acc: 0.5699\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.567049808429\n",
      "Kappa:\n",
      "0.385094114658\n",
      "Confusion matrix:\n",
      "[[134   0  66   0   0]\n",
      " [  4 217 104   0   0]\n",
      " [ 30  66 241   0   0]\n",
      " [ 12   6  77   0   0]\n",
      " [  3  64  20   0   0]]\n",
      "AUC score:\n",
      "0.766953022611\n",
      "('Epoch', 7, '/', 20)\n",
      "Train on 4472 samples, validate on 1044 samples\n",
      "Epoch 1/1\n",
      "4471/4472 [============================>.] - ETA: 0s - loss: 0.9866 - acc: 0.6446Epoch 00000: val_acc improved from 0.59674 to 0.59962, saving model to activity.weights--3lstmbis.best.hdf5\n",
      "4472/4472 [==============================] - 150s - loss: 0.9864 - acc: 0.6447 - val_loss: 1.2076 - val_acc: 0.5996\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.60153256705\n",
      "Kappa:\n",
      "0.45202382154\n",
      "Confusion matrix:\n",
      "[[187   1  12   0   0]\n",
      " [ 24 248  53   0   0]\n",
      " [ 78  63 193   0   3]\n",
      " [ 56  10  29   0   0]\n",
      " [ 13  66   8   0   0]]\n",
      "AUC score:\n",
      "0.794190805017\n",
      "('Epoch', 8, '/', 20)\n",
      "Train on 4472 samples, validate on 1044 samples\n",
      "Epoch 1/1\n",
      "4471/4472 [============================>.] - ETA: 0s - loss: 0.9134 - acc: 0.6773Epoch 00000: val_acc did not improve\n",
      "4472/4472 [==============================] - 149s - loss: 0.9132 - acc: 0.6773 - val_loss: 1.2318 - val_acc: 0.5776\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.592911877395\n",
      "Kappa:\n",
      "0.426594537592\n",
      "Confusion matrix:\n",
      "[[168   3  29   0   0]\n",
      " [  6 170 149   0   0]\n",
      " [ 36  20 281   0   0]\n",
      " [ 24   8  63   0   0]\n",
      " [  4  56  27   0   0]]\n",
      "AUC score:\n",
      "0.799562697817\n",
      "('Epoch', 9, '/', 20)\n",
      "Train on 4472 samples, validate on 1044 samples\n",
      "Epoch 1/1\n",
      "2247/4472 [==============>...............] - ETA: 73s - loss: 0.9535 - acc: 0.6529"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-659056b7d06a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     history = modelA.fit(trainX, dummy_y_trainA, validation_data=(testX, dummy_y_testA), \n\u001b[1;32m     33\u001b[0m                         \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                         verbose=1, callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mmodelA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mkappa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprintValStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_y_testA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1080\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    799\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the model and parameters for training\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "batch = 1\n",
    "epochs = 100\n",
    "\n",
    "modelA3 = create_LSTM3_PCA(dummy_y_trainA.shape[1], batch_size = batch, trainShape1=n_components)\n",
    "print modelA3.summary()\n",
    "\n",
    "# To save the best model\n",
    "# serialize model to JSON\n",
    "modelA3_json = modelA3.to_json()\n",
    "with open(\"activity.model--3lstmbis.json\", \"w\") as json_file:\n",
    "    json_file.write(modelA3_json)\n",
    "filepathA3=\"activity.weights--3lstmbis.best.hdf5\"\n",
    "# Define that the accuracy in cv is monitored, and that weights are stored in a file when max accuracy is achieved\n",
    "checkpointA3 = ModelCheckpoint(filepathA3, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_listA3 = [checkpointA3]\n",
    "\n",
    "# Fit the model\n",
    "accs =[]\n",
    "val_accs =[]\n",
    "losss =[]\n",
    "val_losss =[]\n",
    "kappas = []\n",
    "aucs = []\n",
    "\n",
    "# Manually create epochs and reset between sessions\n",
    "for i in range(epochs):\n",
    "    # Single epoch. Remember to not shuffle the data!\n",
    "    print('Epoch', i+1, '/', epochs)\n",
    "    history = modelA3.fit(trainX, dummy_y_trainA, validation_data=(testX, dummy_y_testA), \n",
    "                        nb_epoch=1, batch_size=batch, shuffle=False, \n",
    "                        verbose=1, callbacks=callbacks_listA3)\n",
    "    modelA3.reset_states()\n",
    "    kappa, auc = printValStats(modelA3, testX, dummy_y_testA, batch=batch)\n",
    "    accs.append(history.history['acc'][0])\n",
    "    val_accs.append(history.history['val_acc'][0])\n",
    "    losss.append(history.history['loss'][0])\n",
    "    val_losss.append(history.history['val_loss'][0])\n",
    "    kappas.append(kappa)\n",
    "    aucs.append(auc)\n",
    "    \n",
    "print 'Best validation accuracy: ', get_max_values(val_accs)\n",
    "plot_training(accs, val_accs, losss, val_losss, kappas, aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-layer (wider) LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the model and parameters for training\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "batch = 1\n",
    "epochs = 100\n",
    "\n",
    "modelA2 = create_LSTM2_PCA(dummy_y_trainA.shape[1], batch_size = batch, trainShape1=n_components)\n",
    "print modelA2.summary()\n",
    "\n",
    "# To save the best model\n",
    "# serialize model to JSON\n",
    "modelA2_json = modelA2.to_json()\n",
    "with open(\"activity.model--2lstmbis.json\", \"w\") as json_file:\n",
    "    json_file.write(modelA2_json)\n",
    "filepathA2=\"activity.weights--2lstmbis.best.hdf5\"\n",
    "# Define that the accuracy in cv is monitored, and that weights are stored in a file when max accuracy is achieved\n",
    "checkpointA2 = ModelCheckpoint(filepathA2, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_listA2 = [checkpointA2]\n",
    "\n",
    "# Fit the model\n",
    "accs =[]\n",
    "val_accs =[]\n",
    "losss =[]\n",
    "val_losss =[]\n",
    "kappas = []\n",
    "aucs = []\n",
    "\n",
    "# Manually create epochs and reset between sessions\n",
    "for i in range(epochs):\n",
    "    # Single epoch. Remember to not shuffle the data!\n",
    "    print('Epoch', i+1, '/', epochs)\n",
    "    history = modelA2.fit(trainX, dummy_y_trainA, validation_data=(testX, dummy_y_testA), \n",
    "                        nb_epoch=1, batch_size=batch, shuffle=False, \n",
    "                        verbose=1, callbacks=callbacks_listA2)\n",
    "    modelA2.reset_states()\n",
    "    kappa, auc = printValStats(modelA2, testX, dummy_y_testA, batch=batch)\n",
    "    accs.append(history.history['acc'][0])\n",
    "    val_accs.append(history.history['val_acc'][0])\n",
    "    losss.append(history.history['loss'][0])\n",
    "    val_losss.append(history.history['val_loss'][0])\n",
    "    kappas.append(kappa)\n",
    "    aucs.append(auc)\n",
    "    \n",
    "print 'Best validation accuracy: ', get_max_values(val_accs)\n",
    "plot_training(accs, val_accs, losss, val_losss, kappas, aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-layer (wider) LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the model and parameters for training\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "batch = 1\n",
    "epochs = 100\n",
    "\n",
    "modelA1 = create_LSTM1_PCA(dummy_y_trainA.shape[1], batch_size = batch, trainShape1=n_components)\n",
    "print modelA1.summary()\n",
    "\n",
    "# To save the best model\n",
    "# serialize model to JSON\n",
    "modelA1_json = modelA1.to_json()\n",
    "with open(\"activity.model--1lstmbis.json\", \"w\") as json_file:\n",
    "    json_file.write(modelA1_json)\n",
    "filepathA1=\"activity.weights--1lstmbis.best.hdf5\"\n",
    "# Define that the accuracy in cv is monitored, and that weights are stored in a file when max accuracy is achieved\n",
    "checkpointA1 = ModelCheckpoint(filepathA1, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_listA1 = [checkpointA1]\n",
    "\n",
    "# Fit the model\n",
    "accs =[]\n",
    "val_accs =[]\n",
    "losss =[]\n",
    "val_losss =[]\n",
    "kappas = []\n",
    "aucs = []\n",
    "\n",
    "# Manually create epochs and reset between sessions\n",
    "for i in range(epochs):\n",
    "    # Single epoch. Remember to not shuffle the data!\n",
    "    print('Epoch', i+1, '/', epochs)\n",
    "    history = modelA1.fit(trainX, dummy_y_trainA, validation_data=(testX, dummy_y_testA), \n",
    "                        nb_epoch=1, batch_size=batch, shuffle=False, \n",
    "                        verbose=1, callbacks=callbacks_listA1)\n",
    "    modelA1.reset_states()\n",
    "    kappa, auc = printValStats(modelA1, testX, dummy_y_testA, batch=batch)\n",
    "    accs.append(history.history['acc'][0])\n",
    "    val_accs.append(history.history['val_acc'][0])\n",
    "    losss.append(history.history['loss'][0])\n",
    "    val_losss.append(history.history['val_loss'][0])\n",
    "    kappas.append(kappa)\n",
    "    aucs.append(auc)\n",
    "    \n",
    "print 'Best validation accuracy: ', get_max_values(val_accs)\n",
    "plot_training(accs, val_accs, losss, val_losss, kappas, aucs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
