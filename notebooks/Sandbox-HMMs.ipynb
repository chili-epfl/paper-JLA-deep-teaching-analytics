{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, cohen_kappa_score, f1_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from math import ceil, sqrt\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import model_from_json\n",
    "import numpy\n",
    "import os\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from hmmlearn.hmm import GaussianHMM, GMMHMM\n",
    "import xgboost\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pandas.read_csv(\"../data/processed/train.csv\")\n",
    "notnull_data = data[data.notnull().all(axis=1)]\n",
    "train = notnull_data.values\n",
    "data2 = pandas.read_csv(\"../data/processed/test.csv\")\n",
    "notnull_data2 = data2[data2.notnull().all(axis=1)]\n",
    "test = notnull_data2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing dataset with PCA -- to %d variance 0.8\n",
      "Total variance explained by %d components:  426\n",
      "0.800088479971\n"
     ]
    }
   ],
   "source": [
    "# Separate the target values (Activity and Social) from features, etc.\n",
    "X_train = train[:,3:7558].astype(float)\n",
    "Y_trainA = train[:,7558] #Activity\n",
    "Y_trainS = train[:,7559] #Social\n",
    "X_test = test[:,3:7558].astype(float)\n",
    "Y_testA = test[:,7558]\n",
    "Y_testS = test[:,7559]\n",
    "\n",
    "# encode class values as integers\n",
    "encoderA = LabelEncoder()\n",
    "encoderA.fit(Y_trainA)\n",
    "encoded_Y_trainA = encoderA.transform(Y_trainA)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_trainA = to_categorical(encoded_Y_trainA)\n",
    "encoderA.fit(Y_testA)\n",
    "encoded_Y_testA = encoderA.transform(Y_testA)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_testA = to_categorical(encoded_Y_testA)\n",
    "\n",
    "# encode class values as integers\n",
    "encoderS = LabelEncoder()\n",
    "encoderS.fit(Y_trainS)\n",
    "encoded_Y_trainS = encoderS.transform(Y_trainS)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_trainS = to_categorical(encoded_Y_trainS)\n",
    "encoderS.fit(Y_testS)\n",
    "encoded_Y_testS = encoderS.transform(Y_testS)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_testS = to_categorical(encoded_Y_testS)\n",
    "\n",
    "# We standardize on the basis of the training data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_st = scaler.transform(X_train)\n",
    "X_test_st = scaler.transform(X_test)\n",
    "\n",
    "# Number of components to extract from the dataset\n",
    "n_components = 100\n",
    "\n",
    "#print 'Reducing dataset with PCA --',n_components\n",
    "#pca = decomposition.PCA(n_components=n_components)\n",
    "perc_variance = 0.8\n",
    "print('Reducing dataset with PCA -- to %d variance', perc_variance)\n",
    "pca = decomposition.PCA(n_components=perc_variance)\n",
    "X_train_pca = pca.fit_transform(X_train_st)\n",
    "X_test_pca = pca.transform(X_test_st)\n",
    "\n",
    "#print 'Variance explained:'\n",
    "#print pca.explained_variance_ratio_\n",
    "print('Total variance explained by %d components: ',pca.n_components_)\n",
    "print(sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic SVM and XGBoost\n",
    "\n",
    "## All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the data...done\n"
     ]
    }
   ],
   "source": [
    "X = X_train_st\n",
    "Y = Y_trainA\n",
    "Xtest = X_test_st\n",
    "Ytest = Y_testA\n",
    "\n",
    "modelSVM = SVC()\n",
    "modelXGB = xgboost.XGBClassifier()\n",
    "\n",
    "print(\"Training the data...\", end=\"\")\n",
    "\n",
    "modelSVM.fit(X,Y)\n",
    "modelXGB.fit(X,Y)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "Accuracy:  0.532567049808\n",
      "Kappa:  0.3699012688\n",
      "F1 macro:  0.461764195905\n",
      "F1 weighted:  0.513931923736\n",
      "AUC:  0.673024006504\n",
      "SVM\n",
      "Accuracy:  0.593869731801\n",
      "Kappa:  0.447808541294\n",
      "F1 macro:  0.505375511432\n",
      "F1 weighted:  0.572504799314\n",
      "AUC:  0.698861888467\n"
     ]
    }
   ],
   "source": [
    "# Test it on validation dataset, and get the different performance metrics\n",
    "models = {'SVM': modelSVM, 'XGBoost': modelXGB}\n",
    "\n",
    "\n",
    "for name, model in models.iteritems():\n",
    "    print(name)\n",
    "    predictions = model.predict(Xtest)\n",
    "    print(\"Accuracy: \",accuracy_score(Ytest, predictions))\n",
    "    print(\"Kappa: \",cohen_kappa_score(Ytest, predictions))\n",
    "    print(\"F1 macro: \",f1_score(Ytest, predictions, average='macro'))\n",
    "    print(\"F1 weighted: \",f1_score(Ytest, predictions, average='weighted'))\n",
    "    encoded_predictions = encoderA.transform(predictions)\n",
    "    dummy_predictions = to_categorical(encoded_predictions)\n",
    "    print(\"AUC: \",roc_auc_score(dummy_y_testA, dummy_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the data...done\n"
     ]
    }
   ],
   "source": [
    "# Fit with PCA features\n",
    "\n",
    "X = X_train_pca\n",
    "Y = Y_trainA\n",
    "Xtest = X_test_pca\n",
    "Ytest = Y_testA\n",
    "\n",
    "modelSVMPCA = SVC()\n",
    "modelXGBPCA = xgboost.XGBClassifier()\n",
    "\n",
    "print(\"Training the data...\", end=\"\")\n",
    "\n",
    "modelSVMPCA.fit(X,Y)\n",
    "modelXGBPCA.fit(X,Y)\n",
    "\n",
    "print(\"done\")\n",
    "# Add models to dictionary\n",
    "models['SVMPCA'] = modelSVMPCA\n",
    "models['XGBPCA'] = modelXGBPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBPCA\n",
      "Accuracy:  0.534482758621\n",
      "Kappa:  0.371441172025\n",
      "F1 macro:  0.436350481854\n",
      "F1 weighted:  0.508069864979\n",
      "AUC:  0.666185477209\n",
      "SVMPCA\n",
      "Accuracy:  0.311302681992\n",
      "Kappa:  0.0\n",
      "F1 macro:  0.0949598246896\n",
      "F1 weighted:  0.147806240537\n",
      "AUC:  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Test it on validation dataset, and get the different performance metrics\n",
    "models = {'SVMPCA': modelSVMPCA, 'XGBPCA': modelXGBPCA}\n",
    "\n",
    "for name, model in models.iteritems():\n",
    "    print(name)\n",
    "    predictions = model.predict(Xtest)\n",
    "    print(\"Accuracy: \",accuracy_score(Ytest, predictions))\n",
    "    print(\"Kappa: \",cohen_kappa_score(Ytest, predictions))\n",
    "    print(\"F1 macro: \",f1_score(Ytest, predictions, average='macro'))\n",
    "    print(\"F1 weighted: \",f1_score(Ytest, predictions, average='weighted'))\n",
    "    encoded_predictions = encoderA.transform(predictions)\n",
    "    dummy_predictions = to_categorical(encoded_predictions)\n",
    "    print(\"AUC: \",roc_auc_score(dummy_y_testA, dummy_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With HMMs\n",
    "\n",
    "## From raw features - then SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting to HMM and decoding 2 ...done\n",
      "fitting to HMM and decoding 3 ...done\n",
      "fitting to HMM and decoding 4 ...done\n",
      "fitting to HMM and decoding 5 ...done\n",
      "fitting to HMM and decoding 6 ...done\n",
      "fitting to HMM and decoding 7 ...done\n",
      "fitting to HMM and decoding 8 ...done\n",
      "fitting to HMM and decoding 9 ...done\n",
      "fitting to HMM and decoding 10 ...done\n",
      "fitting to HMM and decoding 11 ...done\n",
      "fitting to HMM and decoding 12 ...done\n",
      "fitting to HMM and decoding 13 ...done\n",
      "fitting to HMM and decoding 14 ...done\n",
      "fitting to HMM and decoding 15 ...done\n",
      "New dataset size (4472, 7674) (1044, 7674)\n"
     ]
    }
   ],
   "source": [
    "X = X_train_st\n",
    "Y = Y_trainA\n",
    "Xtest = X_test_st\n",
    "Ytest = Y_testA\n",
    "\n",
    "# # Create a simple 2-state with the raw data\n",
    "# print(\"fitting to HMM and decoding ...\", end=\"\")\n",
    "# # Make an HMM instance and execute fit\n",
    "# modelHMM = GaussianHMM(n_components=2, covariance_type=\"diag\").fit(X)\n",
    "# # Predict the optimal sequence of internal hidden state\n",
    "# hidden_states = modelHMM.predict(X)\n",
    "# print(\"done\")\n",
    "# print(hidden_states.shape)\n",
    "# print(\"Transition matrix\")\n",
    "# print(modelHMM.transmat_)\n",
    "# print()\n",
    "# print(\"Means and vars of each hidden state\")\n",
    "# for i in range(modelHMM.n_components):\n",
    "#     print(\"{0}th hidden state\".format(i))\n",
    "#     print(\"mean = \", modelHMM.means_[i])\n",
    "#     print(\"var = \", numpy.diag(modelHMM.covars_[i]))\n",
    "#     print()\n",
    "\n",
    "newX = X\n",
    "newXtest = Xtest\n",
    "\n",
    "for n_comp in range(2,16):\n",
    "    print(\"fitting to HMM and decoding %d ...\" % n_comp , end=\"\")\n",
    "    modelHMM = GaussianHMM(n_components=n_comp, covariance_type=\"diag\").fit(X)\n",
    "    hidden_states_train = to_categorical(modelHMM.predict(X))\n",
    "    hidden_states_test = to_categorical(modelHMM.predict(Xtest))\n",
    "    print(\"done\")\n",
    "    newX = numpy.column_stack((newX,hidden_states_train))\n",
    "    newXtest = numpy.column_stack((newXtest,hidden_states_test))\n",
    "    \n",
    "print('New dataset size',newX.shape,newXtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the data...done\n"
     ]
    }
   ],
   "source": [
    "modelSVMHMM = SVC()\n",
    "print(\"Training the data...\", end=\"\")\n",
    "modelSVMHMM.fit(newX,Y)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM+SVM on ALL features\n",
      "Accuracy:  0.594827586207\n",
      "Kappa:  0.449310097578\n",
      "F1 macro:  0.503299459974\n",
      "F1 weighted:  0.572533894184\n",
      "AUC:  0.698988529981\n"
     ]
    }
   ],
   "source": [
    "print('HMM+SVM on ALL features')\n",
    "predictions = modelSVMHMM.predict(newXtest)\n",
    "print(\"Accuracy: \",accuracy_score(Ytest, predictions))\n",
    "print(\"Kappa: \",cohen_kappa_score(Ytest, predictions))\n",
    "print(\"F1 macro: \",f1_score(Ytest, predictions, average='macro'))\n",
    "print(\"F1 weighted: \",f1_score(Ytest, predictions, average='weighted'))\n",
    "encoded_predictions = encoderA.transform(predictions)\n",
    "dummy_predictions = to_categorical(encoded_predictions)\n",
    "print(\"AUC: \",roc_auc_score(dummy_y_testA, dummy_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From PCA features - then XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting to HMM and decoding 2 ...done\n",
      "fitting to HMM and decoding 3 ...done\n",
      "fitting to HMM and decoding 4 ...done\n",
      "fitting to HMM and decoding 5 ...done\n",
      "fitting to HMM and decoding 6 ...done\n",
      "fitting to HMM and decoding 7 ...done\n",
      "fitting to HMM and decoding 8 ...done\n",
      "fitting to HMM and decoding 9 ...done\n",
      "fitting to HMM and decoding 10 ...done\n",
      "fitting to HMM and decoding 11 ...done\n",
      "fitting to HMM and decoding 12 ...done\n",
      "fitting to HMM and decoding 13 ...done\n",
      "fitting to HMM and decoding 14 ...done\n",
      "fitting to HMM and decoding 15 ...done\n",
      "New dataset size (4472, 545) (1044, 545)\n"
     ]
    }
   ],
   "source": [
    "X = X_train_pca\n",
    "Y = Y_trainA\n",
    "Xtest = X_test_pca\n",
    "Ytest = Y_testA\n",
    "\n",
    "newX = X\n",
    "newXtest = Xtest\n",
    "\n",
    "for n_comp in range(2,16):\n",
    "    print(\"fitting to HMM and decoding %d ...\" % n_comp , end=\"\")\n",
    "    modelHMM = GaussianHMM(n_components=n_comp, covariance_type=\"diag\").fit(X)\n",
    "    hidden_states_train = to_categorical(modelHMM.predict(X))\n",
    "    hidden_states_test = to_categorical(modelHMM.predict(Xtest))\n",
    "    print(\"done\")\n",
    "    newX = numpy.column_stack((newX,hidden_states_train))\n",
    "    newXtest = numpy.column_stack((newXtest,hidden_states_test))\n",
    "    \n",
    "print('New dataset size',newX.shape,newXtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the data...done\n"
     ]
    }
   ],
   "source": [
    "modelXGBPCAHMM = xgboost.XGBClassifier()\n",
    "print(\"Training the data...\", end=\"\")\n",
    "modelXGBPCAHMM.fit(newX,Y)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM+XGBoost on 426 PCA features\n",
      "Accuracy:  0.519157088123\n",
      "Kappa:  0.35603225196\n",
      "F1 macro:  0.428643743077\n",
      "F1 weighted:  0.490372954964\n",
      "AUC:  0.666712371431\n"
     ]
    }
   ],
   "source": [
    "print('HMM+XGBoost on %d PCA features' % X_train_pca.shape[1])\n",
    "predictions = modelXGBPCAHMM.predict(newXtest)\n",
    "print(\"Accuracy: \",accuracy_score(Ytest, predictions))\n",
    "print(\"Kappa: \",cohen_kappa_score(Ytest, predictions))\n",
    "print(\"F1 macro: \",f1_score(Ytest, predictions, average='macro'))\n",
    "print(\"F1 weighted: \",f1_score(Ytest, predictions, average='weighted'))\n",
    "encoded_predictions = encoderA.transform(predictions)\n",
    "dummy_predictions = to_categorical(encoded_predictions)\n",
    "print(\"AUC: \",roc_auc_score(dummy_y_testA, dummy_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No advantage, apparently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
