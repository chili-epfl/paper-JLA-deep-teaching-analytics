{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(505693, 9)\n",
      "   Unnamed: 0  accelerationX  accelerationY  accelerationZ  timestamp  \\\n",
      "1         621         -0.306          6.704          7.834       60.0   \n",
      "2          63         -0.383          6.771          6.799      121.0   \n",
      "3         642         -0.162          6.416          7.010      183.0   \n",
      "4         655         -0.249          6.445          6.962      241.0   \n",
      "\n",
      "                        session  timestamp.orig Activity Social  \n",
      "1  case1-day1-session1-teacher1   1433229445693      NaN    NaN  \n",
      "2  case1-day1-session1-teacher1   1433229445754      TDT    CLS  \n",
      "3  case1-day1-session1-teacher1   1433229445816      TDT    CLS  \n",
      "4  case1-day1-session1-teacher1   1433229445874      TDT    CLS  \n",
      "(411852, 2)\n",
      "(411852, 3)\n",
      "(93841,)\n",
      "(93841, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "numpy.random.seed(7)\n",
    "\n",
    "data = pandas.read_csv(\"../data/interim/accelData.csv\")\n",
    "print data.shape\n",
    "print data[1:5] #Data is separated 60ms\n",
    "#plt.plot(data[['accelerationX','accelerationY','accelerationZ']][1:100])\n",
    "#plt.show()\n",
    "\n",
    "# We clean up the Activity and social values\n",
    "cleandata = data\n",
    "cleandata.loc[cleandata['Activity'].isnull(),'Activity'] = 'Other'\n",
    "cleandata.loc[cleandata['Activity'] == 'OFF','Activity'] = 'Other'\n",
    "cleandata.loc[cleandata['Activity'] == 'TEC','Activity'] = 'Other'\n",
    "cleandata.loc[cleandata['Activity'] == 'TDT','Activity'] = 'Other'\n",
    "cleandata.loc[cleandata['Social'].isnull(),'Social'] = 'Other'\n",
    "#print numpy.unique(cleandata['Activity']), numpy.unique(cleandata['Social'])\n",
    "#print cleandata.shape\n",
    "\n",
    "cleandata = cleandata[cleandata.notnull().all(axis=1)]\n",
    "#print 'Not null data'\n",
    "#print cleandata.shape\n",
    "\n",
    "train = cleandata[~cleandata['session'].isin(['case1-day1-session1-teacher1','case2-day3-session1-teacher2'])]\n",
    "#print train.shape\n",
    "test = cleandata[cleandata['session'].isin(['case1-day1-session1-teacher1','case2-day3-session1-teacher2'])]\n",
    "#print test.shape\n",
    "\n",
    "# We split our datasets into session+timestamps, X and Y\n",
    "times_train = train.loc[:,['session','timestamp']]\n",
    "times_test = test.loc[:,['session','timestamp']]\n",
    "\n",
    "X_train = train.loc[:,['accelerationX','accelerationY','accelerationZ']].astype(float)\n",
    "Y_train = train.loc[:,'Activity'] # Social is 8\n",
    "\n",
    "X_test = test.loc[:,['accelerationX','accelerationY','accelerationZ']].astype(float)\n",
    "Y_test = test.loc[:,'Activity']\n",
    "\n",
    "# One hot encoding of the response variable (using dummy variables)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_train)\n",
    "encoded_Y_train = encoder.transform(Y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_train = to_categorical(encoded_Y_train)\n",
    "encoder.fit(Y_test)\n",
    "encoded_Y_test = encoder.transform(Y_test)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_test = to_categorical(encoded_Y_test)\n",
    "\n",
    "# Sanity check on matrix dimensions, after droppinig null/nans\n",
    "print times_train.shape #\n",
    "print X_train.shape #\n",
    "print Y_test.shape #\n",
    "print dummy_y_test.shape #\n",
    "\n",
    "#print 'X before normalization'\n",
    "#print X_train[1:5]\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#print 'X after normalization'\n",
    "#print X_train[1:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(411852, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "testX = numpy.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "print trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_15 (LSTM)                     (1, 1, 100)         41600       lstm_input_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)               (1, 1, 100)         0           lstm_15[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                     (1, 50)             30200       dropout_24[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)               (1, 50)             0           lstm_16[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                   (1, 50)             2550        dropout_25[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)               (1, 50)             0           dense_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                   (1, 5)              255         dropout_26[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 74605\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def create_LSTM2Acc(batch_size = 1, trainShape1=100):\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    # stateful LSTM!\n",
    "    #model.add(LSTM(200, batch_input_shape=(batch_size, 1, trainShape1), \n",
    "    #               return_sequences=True, stateful=True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100, batch_input_shape=(batch_size, 1, trainShape1), \n",
    "                   return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, \n",
    "                   return_sequences=False, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    #model.add(Dense(20, activation='tanh'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate baseline model with standardized dataset\n",
    "numpy.random.seed(66)\n",
    "#estimators = []\n",
    "#estimators.append(('standardize', StandardScaler()))\n",
    "#estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, nb_epoch=10, batch_size=10, verbose=1)))\n",
    "# We define a pipeline of estimators, in which first the scaler is fitted to the data, then the MLP is applied\n",
    "#pipeline = Pipeline(estimators)\n",
    "#kfold = StratifiedKFold(y=Y_train, n_folds=3, shuffle=True, random_state=seed)\n",
    "\n",
    "batch_size = 1\n",
    "nb_epochs = 3\n",
    "print trainX.shape[2]\n",
    "\n",
    "#model = create_baseline()\n",
    "model = create_LSTM2Acc(batch_size=batch_size, trainShape1=trainX.shape[2])\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', 1, '/', 3)\n",
      "Train on 411852 samples, validate on 93841 samples\n",
      "Epoch 1/1\n",
      "411851/411852 [============================>.] - ETA: 0s - loss: 0.2801 - acc: 0.9534Epoch 00000: val_acc improved from -inf to 0.41287, saving model to acc.weights--2lstm.best.hdf5\n",
      "411852/411852 [==============================] - 10243s - loss: 0.2801 - acc: 0.9534 - val_loss: 7.2106 - val_acc: 0.4129\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.412868575569\n",
      "Confusion matrix:\n",
      "[[    0     0 16008     0     0]\n",
      " [    0     0 24517     0     0]\n",
      " [    0     0 38744     0     0]\n",
      " [    0     0  7769     0     0]\n",
      " [    0     0  6803     0     0]]\n",
      "AUC score:\n",
      "0.499916350484\n",
      "('Epoch', 2, '/', 3)\n",
      "Train on 411852 samples, validate on 93841 samples\n",
      "Epoch 1/1\n",
      "182230/411852 [============>.................] - ETA: 6254s - loss: 0.3251 - acc: 0.9531"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-da51cd5256be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     history = model.fit(trainX, dummy_y_train, validation_data=(testX,dummy_y_test), \n\u001b[1;32m     49\u001b[0m                         \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                         verbose=1, callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mval_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1050\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score\n",
    "\n",
    "# To save the best model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"acc.model--2lstm.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "filepath=\"acc.weights--2lstm.best.hdf5\"\n",
    "# Define that the accuracy in cv is monitored, and that weights are stored in a file when max accuracy is achieved\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "def printValStats(model, testX, dummy_y_test):\n",
    "    # Other performance/accuracy metrics\n",
    "    Y_pred = model.predict(testX, batch_size=batch_size)\n",
    "    model.reset_states()\n",
    "    print 'Performance of model on test set ----------------------------'\n",
    "    # Accuracy\n",
    "    print('Accuracy:')\n",
    "    print(accuracy_score(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1)))\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1))\n",
    "    numpy.set_printoptions(precision=2)\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    # AUC\n",
    "    roc = roc_auc_score(dummy_y_test, Y_pred, average='macro')\n",
    "    print('AUC score:')\n",
    "    print(roc)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "accs =[]\n",
    "val_accs =[]\n",
    "losss =[]\n",
    "val_losss =[]\n",
    "\n",
    "# Manually create epochs and reset between sessions\n",
    "for i in range(nb_epochs):\n",
    "    # Single epoch. Remember to not shuffle the data!\n",
    "    print('Epoch', i+1, '/', nb_epochs)\n",
    "    #print trainX[0:5,:,:]\n",
    "    #print dummy_y_train[0:5,:]\n",
    "    history = model.fit(trainX, dummy_y_train, validation_data=(testX,dummy_y_test), \n",
    "                        nb_epoch=1, batch_size=batch_size, shuffle=False, \n",
    "                        verbose=1, callbacks=callbacks_list)\n",
    "    accs.append(history.history['acc'][0])\n",
    "    val_accs.append(history.history['val_acc'][0])\n",
    "    losss.append(history.history['loss'][0])\n",
    "    val_losss.append(history.history['val_loss'][0])\n",
    "\n",
    "    model.reset_states()\n",
    "    printValStats(model, testX, dummy_y_test)\n",
    "\n",
    "import operator\n",
    "index, value = max(enumerate(val_accs), key=operator.itemgetter(1))\n",
    "print index, value\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(accs)\n",
    "plt.plot(val_accs)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(losss)\n",
    "plt.plot(val_losss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For comparison, do RF of accel dataset\n",
    "\n",
    "Both on the **raw data**, and on the **10s data** (accel features only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the random forest package\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "# Create the random forest object which will include all the parameters\n",
    "# for the fit\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fit the training data to the Survived labels and create the decision trees\n",
    "forest = forest.fit(train_data[0::,1::],train_data[0::,0])\n",
    "\n",
    "# Take the same decision trees and run it on the test data\n",
    "print 'Accuracy on test data (RAW):',forest.score(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
