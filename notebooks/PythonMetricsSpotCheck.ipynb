{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from keras.models import model_from_json\n",
    "import os\n",
    "\n",
    "# LOAD AND USE MODEL\n",
    "json_file = open('../src/models/LSTM-100PCA-DeepDrop_all_GM_LOSO_Social_1.model.json','r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"../src/models/LSTM-100PCA-DeepDrop_all_GM_LOSO_Social_1.weights.hdf5\")\n",
    "print(\"Loaded model from disk\")\n",
    "# evaluate loaded model on test data\n",
    "# IMPORTANT: compile the model again before use!\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: 7559\n",
      "(array(['session', 'timestamp', 'value.Mean', 'value.SD', 'value.Fix'], dtype=object), '...', array(['V998', 'V999', 'V1000', 'Activity', 'Social'], dtype=object))\n",
      "(5561, 7558)\n",
      "(461, 7558)\n",
      "(5100, 7558)\n",
      "Removing null and NAs...\n",
      "(455, 7558)\n",
      "(5061, 7558)\n",
      "((5061, 7555), (5061,), (455, 7555), (455,))\n",
      "Transforming data... \n",
      "PCA with 100 components\n",
      "Variance explained:\n",
      "[ 0.19377148  0.08003752  0.03967616  0.02126597  0.01905889  0.01672362\n",
      "  0.01431736  0.01164806  0.0111452   0.00970049  0.00875957  0.00787292\n",
      "  0.00725064  0.00706045  0.00651681  0.00596103  0.00536422  0.00535148\n",
      "  0.00511501  0.00460986  0.00443447  0.00423327  0.00392198  0.0034854\n",
      "  0.00345562  0.00337559  0.00322898  0.00305009  0.00304195  0.00295511\n",
      "  0.00283019  0.0027339   0.00266283  0.00257872  0.00254498  0.00248343\n",
      "  0.00235735  0.00232665  0.0022771   0.00223077  0.00220502  0.00218941\n",
      "  0.00212522  0.00208172  0.00202671  0.00195109  0.00193412  0.00189897\n",
      "  0.00186267  0.00180383  0.00178489  0.00173409  0.00168005  0.00166644\n",
      "  0.00165429  0.00161881  0.00160262  0.00157611  0.00154179  0.00154021\n",
      "  0.00152865  0.00151852  0.00146609  0.00145761  0.00144716  0.00142183\n",
      "  0.0014066   0.00139206  0.00136871  0.00136297  0.00134052  0.0013204\n",
      "  0.0013035   0.00128509  0.00127124  0.00124574  0.0012419   0.00122652\n",
      "  0.00121926  0.0012032   0.0011932   0.00118808  0.00117546  0.0011523\n",
      "  0.00114439  0.00113038  0.00111617  0.00110789  0.00107763  0.00106804\n",
      "  0.00105303  0.00104948  0.00103103  0.00102059  0.00100962  0.00100413\n",
      "  0.00099344  0.00097714  0.0009564   0.00095332]\n",
      "Total variance explained by 100 components:\n",
      "0.626322457607\n"
     ]
    }
   ],
   "source": [
    "from sklearn import decomposition\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import math\n",
    "\n",
    "#Getting to load the train/test data itself\n",
    "# For tests\n",
    "label='LSTM-100PCA-DeepDrop_et_GM_LOSO_Social_1'\n",
    "target='Social'\n",
    "datasourcestring='all'\n",
    "trainstring='case1-day1-session2-teacher1,case1-day1-session3-teacher1,case1-day1-session4-teacher1,case2-day1-session1-teacher2,case2-day1-session2-teacher2,case2-day2-session1-teacher2,case2-day2-session2-teacher2,case2-day3-session1-teacher2,case2-day3-session2-teacher2,case2-day4-session1-teacher2,case2-day4-session2-teacher2'\n",
    "teststring='case1-day1-session1-teacher1'\n",
    "\n",
    "\n",
    "\n",
    "# We parse the data sources to take into account, and sessions for the train and test sets\n",
    "features = range(0,2) # These are only the session and timestamp\n",
    "sources = datasourcestring.split(\",\")\n",
    "for source in sources:\n",
    "    if(source=='all'):\n",
    "        features.extend(range(2,7557))\n",
    "        break\n",
    "    elif(source=='et'):\n",
    "        features.extend(range(2,12))\n",
    "    elif(source=='acc'):\n",
    "        features.extend(range(12,152))\n",
    "    elif(source=='aud'):\n",
    "        features.extend(range(152,6557))\n",
    "    elif(source=='vid'):\n",
    "        features.extend(range(6557,7557))\n",
    "    else:\n",
    "        sys.exit(\"Wrong data sources. Possible values: all,et,acc,aud,vid\")\n",
    "features.extend(range(7557,7559)) # Add activity and Social\n",
    "print(\"Selected features: \"+str(len(features)))\n",
    "\n",
    "sessiontrain = trainstring.split(\",\") # Gives an array of the sessions to train in\n",
    "sessiontest = teststring.split(\",\") # Gives an array of the sessions to train in\n",
    "\n",
    "if(len(sessiontrain)==0 | len(sessiontest)==0):\n",
    "    sys.exit(\"Wrong train/test sessions specification. Should be a comma-separated string with the sessions identificators\")\n",
    "\n",
    "#path = os.path.dirname(os.path.realpath(sys.argv[0]))\n",
    "# READING AND PREPARING THE DATA\n",
    "#processeddatadir = path\n",
    "processeddatadir = \"../src/models\" # TODO: Change this for the actual script\n",
    "datafile = os.path.join(processeddatadir,'completeDataset.csv')\n",
    "gzdatafile = os.path.join(processeddatadir,'completeDataset.csv.gz')\n",
    "fulldata = pandas.DataFrame()\n",
    "if(os.path.isfile(datafile)):\n",
    "    fulldata = pandas.read_csv(datafile, sep=',', quotechar='\"')\n",
    "elif(os.path.isfile(gzdatafile)):\n",
    "    fulldata = pandas.read_csv(gzdatafile, compression='gzip', sep=',', quotechar='\"')\n",
    "else:\n",
    "    sys.exit(\"Data not available in the script's folder\")\n",
    "\n",
    "# Drop the useless first column\n",
    "fulldata.drop(fulldata.columns[[0]],axis=1,inplace=True)\n",
    "\n",
    "def cleanAct(value):\n",
    "    if pandas.isnull(value):\n",
    "        return 'Other'\n",
    "    elif value=='OFF' or value=='TDT' or value=='TEC':\n",
    "        return 'Other'\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "def cleanSoc(value):\n",
    "    if pandas.isnull(value):\n",
    "        return 'Other'\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "# We only look for predicting 4 states of activity and 3 of social, the rest (incl.NA) we bunch in 'Other' (so in the end it is a 5- and 4-class classification problem)\n",
    "fulldata['Activity'] = fulldata['Activity.win'].map(cleanAct)\n",
    "fulldata['Social'] = fulldata['Social.win'].map(cleanSoc)\n",
    "\n",
    "# Drop the useless first column\n",
    "fulldata.drop(fulldata.columns[[2,3,4]],axis=1,inplace=True)\n",
    "print(fulldata.columns.values[0:5],\"...\",fulldata.columns.values[-5:])\n",
    "#fulldata.head(3)\n",
    "\n",
    "# Now the column indices match what is expected in the arguments parsed above\n",
    "# * [,0]: ''session id''\n",
    "# * [,1]: ''timestamp'' within the session (in ms)\n",
    "# * [,2:12]: ''eyetracking'' features (mean/sd pupil diameter, nr. of long fixations, avg. saccade speed, fixation duration, fixation dispersion, saccade duration, saccade amplitude, saccade length, saccade velocity)\n",
    "# * [,12:152]: ''accelerometer'' features, including X, Y, Z (mean, sd, max, min, median, and 30 FFT coefficients of each of them) and jerk (mean, sd, max, min, median, and 30 FFT coefficients of each of it)\n",
    "# * [,152:6557]: ''audio'' features extracted from an audio snippet of the 10s window, using openSMILE. Includes features about whether there is someone speaking (153:163), emotion recognition models (164:184), and brute-force audio spectrum features and characteristics used in various audio recognition challenges/tasks (185:6557)\n",
    "# * [,6557:7557]: ''video'' features extracted from an image taken in the middle of the window (the 1000 values of the last layer when passing the immage through a VGG pre-trained model)\n",
    "# * [,7557:7559]: ''Activity,Social'' labels we want to predict\n",
    "\n",
    "\n",
    "# SELECTING THE DATASET FEATURES (DATA SOURCES BEING TRIED)\n",
    "data = fulldata.ix[:,features]\n",
    "\n",
    "# We drop the non-needed target variable\n",
    "if target == 'Activity':\n",
    "    data.drop('Social',axis=1,inplace=True)\n",
    "elif target == 'Social':\n",
    "    data.drop('Activity',axis=1,inplace=True)\n",
    "\n",
    "print(data.shape)\n",
    "#data.head(3)\n",
    "\n",
    "# SPLITTING THE DATA\n",
    "test = data.loc[data['session'].isin(sessiontest)]\n",
    "train = data.loc[data['session'].isin(sessiontrain)]\n",
    "print(test.shape)\n",
    "print(train.shape)\n",
    "# Removing null values\n",
    "test = test[test.notnull().all(axis=1)]\n",
    "train = train[train.notnull().all(axis=1)]\n",
    "print(\"Removing null and NAs...\")\n",
    "print(test.shape)\n",
    "print(train.shape)\n",
    "\n",
    "\n",
    "X_train = train.values[:,range(2,train.shape[1]-1)].astype(float)\n",
    "Y_train = train.values[:,(train.shape[1]-1)]\n",
    "X_test = test.values[:,range(2,test.shape[1]-1)].astype(float)\n",
    "Y_test = test.values[:,(test.shape[1]-1)]\n",
    "Y_total = data.values[:,(data.shape[1]-1)]\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
    "\n",
    "#######################################################\n",
    "# DO OTHER DATA TRANSFORMATIONS NEEDED, e.g. PCA, SELECT K-BEST FEATURES, etc (NORMALLY, ON THE TRAIN SET ONLY, TO BE APPLIED LATER TO THE TEST SET)\n",
    "print(\"Transforming data... \")\n",
    "k = 100\n",
    "outs = len(data[target].unique())\n",
    "\n",
    "# We standardize on the basis of the training data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_st = scaler.transform(X_train)\n",
    "X_test_st = scaler.transform(X_test)\n",
    "\n",
    "# # Removing zero variance features\n",
    "# selector = VarianceThreshold()\n",
    "# selector.fit(X_train_st)\n",
    "# X_train_nz = selector.transform(X_train_st)\n",
    "# X_test_nz = selector.transform(X_test_st)\n",
    "# idx = numpy.where(selector.variances_ > threshold)[0] # to get the indices\n",
    "# TODO: Remove highly correlated ones (according to Cohen's d?)\n",
    "## From http://lucystatistics.blogspot.com.ee/2016/03/dimension-reduction.html\n",
    "# c = df.corr().abs()\n",
    "# so1=argsort(np.array(c))\n",
    "# s = c.unstack()\n",
    "# so2 = s.order(kind=\"quicksort\")\n",
    "\n",
    "\n",
    "if X_train.shape[1]>k:\n",
    "    # Apply 100-component pca\n",
    "    print(\"PCA with \"+str(k)+\" components\")\n",
    "    pca = decomposition.PCA(n_components=k)\n",
    "    X_train_pca = pca.fit_transform(X_train_st)\n",
    "    X_test_pca = pca.transform(X_test_st)\n",
    "    print 'Variance explained:'\n",
    "    print pca.explained_variance_ratio_\n",
    "    print 'Total variance explained by '+str(k)+' components:'\n",
    "    print sum(pca.explained_variance_ratio_)\n",
    "else:\n",
    "    k=X_train.shape[1]\n",
    "    pca = decomposition.PCA(n_components=k)\n",
    "    X_train_pca = pca.fit_transform(X_train_st)\n",
    "    X_test_pca = pca.transform(X_test_st)\n",
    "\n",
    "#######################################################\n",
    "\n",
    "\n",
    "# PREPARING THE DATA FOR KERAS TRAINING\n",
    "# One hot encoding of the response variable (using dummy variables)\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_total)\n",
    "encoded_Y_train = encoder.transform(Y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_train = to_categorical(encoded_Y_train)\n",
    "#encoder.fit(Y_test)\n",
    "encoded_Y_test = encoder.transform(Y_test)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_test = to_categorical(encoded_Y_test)\n",
    "\n",
    "\n",
    "seed = 66\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "\n",
    "def slice_timeseries(X,Y,length=32):\n",
    "    # TODO: consider more randomized slicing\n",
    "    n = int(math.floor(X.shape[0] / length))\n",
    "    maxt = n * length\n",
    "    X = X[0:maxt,:].reshape((n, length, X.shape[1]))\n",
    "    Y = Y[0:maxt,:].reshape((n, length, Y.shape[1]))\n",
    "    return X,Y\n",
    "\n",
    "X_train_pca_reshaped, dummy_y_train_reshaped = slice_timeseries(X_train_pca, dummy_y_train, length=16)\n",
    "X_test_pca_reshaped, dummy_y_test_reshaped = slice_timeseries(X_test_pca, dummy_y_test, length=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.81978021978\n",
      "Confusion matrix:\n",
      "[[217  34   0   2]\n",
      " [ 17 141   0   4]\n",
      " [  0  10   0   0]\n",
      " [ 14   1   0  15]]\n",
      "AUC score:\n",
      "0.760041693401\n",
      "F1 score:\n",
      "0.566211896693\n",
      "Kappa score:\n",
      "0.67133255226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score, cohen_kappa_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "Y_pred = loaded_model.predict(X_test_pca.reshape((1,) + X_test_pca.shape ))[0,:,:]\n",
    "\n",
    "# Accuracy\n",
    "print('Accuracy:')\n",
    "acc = accuracy_score(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1))\n",
    "print(acc)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1))\n",
    "numpy.set_printoptions(precision=2)\n",
    "print('Confusion matrix:')\n",
    "print(cm)\n",
    "\n",
    "# AUC\n",
    "roc = None\n",
    "try:\n",
    "  roc = roc_auc_score(dummy_y_test, Y_pred, average='macro')\n",
    "except:\n",
    "  pass\n",
    "print('AUC score:')\n",
    "print(roc)\n",
    "\n",
    "# F1\n",
    "f1= f1_score(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1), average='macro')\n",
    "print('F1 score:')\n",
    "print(f1)\n",
    "\n",
    "\n",
    "# KAppa?\n",
    "kappa = cohen_kappa_score(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1))\n",
    "print('Kappa score:')\n",
    "print(kappa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>cm</th>\n",
       "      <th>f1</th>\n",
       "      <th>kappa</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 0.789011</td>\n",
       "      <td> 0.796477</td>\n",
       "      <td> [[218, 35, 0, 0], [33, 128, 0, 1], [3, 6, 0, 1...</td>\n",
       "      <td> 0.545449</td>\n",
       "      <td> 0.605833</td>\n",
       "      <td> LSTM-100PCA-DeepDrop_all_GM_LOSO_Social_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc       auc                                                 cm  \\\n",
       "0  0.789011  0.796477  [[218, 35, 0, 0], [33, 128, 0, 1], [3, 6, 0, 1...   \n",
       "\n",
       "         f1     kappa                                      label  \n",
       "0  0.545449  0.605833  LSTM-100PCA-DeepDrop_all_GM_LOSO_Social_1  \n",
       "\n",
       "[1 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the performances calculated while training\n",
    "perfdata = pandas.read_csv('../src/models/LSTM-100PCA-DeepDrop_all_GM_LOSO_Social_1.perf.csv', sep=',', quotechar='\"')\n",
    "perfdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might need to recalculate all the python performances!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
