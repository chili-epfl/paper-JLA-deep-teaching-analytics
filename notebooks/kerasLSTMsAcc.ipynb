{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(505693, 9)\n",
      "   Unnamed: 0  accelerationX  accelerationY  accelerationZ  timestamp  \\\n",
      "1         621         -0.306          6.704          7.834       60.0   \n",
      "2          63         -0.383          6.771          6.799      121.0   \n",
      "3         642         -0.162          6.416          7.010      183.0   \n",
      "4         655         -0.249          6.445          6.962      241.0   \n",
      "\n",
      "                        session  timestamp.orig Activity Social  \n",
      "1  case1-day1-session1-teacher1   1433229445693      NaN    NaN  \n",
      "2  case1-day1-session1-teacher1   1433229445754      TDT    CLS  \n",
      "3  case1-day1-session1-teacher1   1433229445816      TDT    CLS  \n",
      "4  case1-day1-session1-teacher1   1433229445874      TDT    CLS  \n",
      "(411852, 2)\n",
      "(411852, 3)\n",
      "(93841,)\n",
      "(93841, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "numpy.random.seed(7)\n",
    "\n",
    "data = pandas.read_csv(\"../data/interim/accelData.csv\")\n",
    "print data.shape\n",
    "print data[1:5] #Data is separated 60ms\n",
    "#plt.plot(data[['accelerationX','accelerationY','accelerationZ']][1:100])\n",
    "#plt.show()\n",
    "\n",
    "# We clean up the Activity and social values\n",
    "cleandata = data\n",
    "cleandata.loc[cleandata['Activity'].isnull(),'Activity'] = 'Other'\n",
    "cleandata.loc[cleandata['Activity'] == 'OFF','Activity'] = 'Other'\n",
    "cleandata.loc[cleandata['Activity'] == 'TEC','Activity'] = 'Other'\n",
    "cleandata.loc[cleandata['Activity'] == 'TDT','Activity'] = 'Other'\n",
    "cleandata.loc[cleandata['Social'].isnull(),'Social'] = 'Other'\n",
    "#print numpy.unique(cleandata['Activity']), numpy.unique(cleandata['Social'])\n",
    "#print cleandata.shape\n",
    "\n",
    "cleandata = cleandata[cleandata.notnull().all(axis=1)]\n",
    "#print 'Not null data'\n",
    "#print cleandata.shape\n",
    "\n",
    "train = cleandata[~cleandata['session'].isin(['case1-day1-session1-teacher1','case2-day3-session1-teacher2'])]\n",
    "#print train.shape\n",
    "test = cleandata[cleandata['session'].isin(['case1-day1-session1-teacher1','case2-day3-session1-teacher2'])]\n",
    "#print test.shape\n",
    "\n",
    "# We split our datasets into session+timestamps, X and Y\n",
    "times_train = train.loc[:,['session','timestamp']]\n",
    "times_test = test.loc[:,['session','timestamp']]\n",
    "\n",
    "X_train = train.loc[:,['accelerationX','accelerationY','accelerationZ']].astype(float)\n",
    "Y_train = train.loc[:,'Activity'] # Social is 8\n",
    "\n",
    "X_test = test.loc[:,['accelerationX','accelerationY','accelerationZ']].astype(float)\n",
    "Y_test = test.loc[:,'Activity']\n",
    "\n",
    "# One hot encoding of the response variable (using dummy variables)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_train)\n",
    "encoded_Y_train = encoder.transform(Y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_train = to_categorical(encoded_Y_train)\n",
    "encoder.fit(Y_test)\n",
    "encoded_Y_test = encoder.transform(Y_test)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_test = to_categorical(encoded_Y_test)\n",
    "\n",
    "# Sanity check on matrix dimensions, after droppinig null/nans\n",
    "print times_train.shape #\n",
    "print X_train.shape #\n",
    "print Y_test.shape #\n",
    "print dummy_y_test.shape #\n",
    "\n",
    "#print 'X before normalization'\n",
    "#print X_train[1:5]\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "#print 'X after normalization'\n",
    "#print X_train[1:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(411852, 1, 3)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_1 (LSTM)                      (1, 20)             1920        lstm_input_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                    (1, 5)              105         lstm_1[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 2025\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "['case1-day1-session2-teacher1' 'case1-day1-session3-teacher1'\n",
      " 'case1-day1-session4-teacher1' 'case2-day1-session1-teacher2'\n",
      " 'case2-day1-session2-teacher2' 'case2-day2-session1-teacher2'\n",
      " 'case2-day2-session2-teacher2' 'case2-day3-session2-teacher2'\n",
      " 'case2-day4-session1-teacher2' 'case2-day4-session2-teacher2']\n"
     ]
    }
   ],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "testX = numpy.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "print trainX.shape\n",
    "\n",
    "# create and fit the LSTM network\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "# stateful LSTM!\n",
    "model.add(LSTM(20, batch_input_shape=(batch_size, 1, X_train.shape[1]), \n",
    "               stateful=True))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print model.summary()\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# To save the best model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"acc.model--1lstm.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "filepath=\"acc.weights--1lstm.best.hdf5\"\n",
    "# Define that the accuracy in cv is monitored, and that weights are stored in a file when max accuracy is achieved\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "nb_epochs = 1\n",
    "sessions = numpy.unique(times_train.session)\n",
    "print sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case1-day1-session2-teacher1 (40739, 1, 3) (40739, 5)\n",
      "(93841, 1, 3) (93841, 5)\n",
      "Train on 40739 samples, validate on 93841 samples\n",
      "Epoch 1/1\n",
      "40738/40739 [============================>.] - ETA: 0s - loss: 8.6945 - acc: 0.4606Epoch 00000: val_acc did not improve\n",
      "40739/40739 [==============================] - 372s - loss: 8.6943 - acc: 0.4606 - val_loss: 9.4634 - val_acc: 0.4129\n",
      "case1-day1-session3-teacher1 (38226, 1, 3) (38226, 5)\n",
      "(93841, 1, 3) (93841, 5)\n",
      "Train on 38226 samples, validate on 93841 samples\n",
      "Epoch 1/1\n",
      "38225/38226 [============================>.] - ETA: 0s - loss: 11.0016 - acc: 0.3174Epoch 00000: val_acc did not improve\n",
      "38226/38226 [==============================] - 352s - loss: 11.0013 - acc: 0.3175 - val_loss: 9.4634 - val_acc: 0.4129\n",
      "case1-day1-session4-teacher1 (35156, 1, 3) (35156, 5)\n",
      "(93841, 1, 3) (93841, 5)\n",
      "Train on 35156 samples, validate on 93841 samples\n",
      "Epoch 1/1\n",
      "35155/35156 [============================>.] - ETA: 0s - loss: 8.3495 - acc: 0.4820Epoch 00000: val_acc did not improve\n",
      "35156/35156 [==============================] - 318s - loss: 8.3493 - acc: 0.4820 - val_loss: 9.4634 - val_acc: 0.4129\n",
      "case2-day1-session1-teacher2 (28453, 1, 3) (28453, 5)\n",
      "(93841, 1, 3) (93841, 5)\n",
      "Train on 28453 samples, validate on 93841 samples\n",
      "Epoch 1/1\n",
      "28452/28453 [============================>.] - ETA: 0s - loss: 6.4711 - acc: 0.5985Epoch 00000: val_acc did not improve\n",
      "28453/28453 [==============================] - 241s - loss: 6.4709 - acc: 0.5985 - val_loss: 9.4634 - val_acc: 0.4129\n",
      "case2-day1-session2-teacher2 (36861, 1, 3) (36861, 5)\n",
      "(93841, 1, 3) (93841, 5)\n",
      "Train on 36861 samples, validate on 93841 samples\n",
      "Epoch 1/1\n",
      "36860/36861 [============================>.] - ETA: 0s - loss: 9.3613 - acc: 0.4192Epoch 00000: val_acc did not improve\n",
      "36861/36861 [==============================] - 304s - loss: 9.3610 - acc: 0.4192 - val_loss: 9.4634 - val_acc: 0.4129\n",
      "case2-day2-session1-teacher2 (44040, 1, 3) (44040, 5)\n",
      "(93841, 1, 3) (93841, 5)\n",
      "Train on 44040 samples, validate on 93841 samples\n",
      "Epoch 1/1\n",
      "44039/44040 [============================>.] - ETA: 0s - loss: 13.0661 - acc: 0.1894Epoch 00000: val_acc did not improve\n",
      "44040/44040 [==============================] - 349s - loss: 13.0658 - acc: 0.1894 - val_loss: 9.4634 - val_acc: 0.4129\n",
      "case2-day2-session2-teacher2 (44581, 1, 3) (44581, 5)\n",
      "(93841, 1, 3) (93841, 5)\n",
      "Train on 44581 samples, validate on 93841 samples\n",
      "Epoch 1/1\n",
      "44580/44581 [============================>.] - ETA: 0s - loss: 13.0901 - acc: 0.1879Epoch 00000: val_acc did not improve\n",
      "44581/44581 [==============================] - 362s - loss: 13.0898 - acc: 0.1879 - val_loss: 9.4634 - val_acc: 0.4129\n",
      "case2-day3-session2-teacher2 (44353, 1, 3) (44353, 5)\n",
      "(93841, 1, 3) (93841, 5)\n",
      "Train on 44353 samples, validate on 93841 samples\n",
      "Epoch 1/1\n",
      "44352/44353 [============================>.] - ETA: 0s - loss: 11.8658 - acc: 0.2638Epoch 00000: val_acc did not improve\n",
      "44353/44353 [==============================] - 367s - loss: 11.8655 - acc: 0.2638 - val_loss: 9.4634 - val_acc: 0.4129\n",
      "case2-day4-session1-teacher2 (51558, 1, 3) (51558, 5)\n",
      "(93841, 1, 3) (93841, 5)\n",
      "Train on 51558 samples, validate on 93841 samples\n",
      "Epoch 1/1\n",
      "51557/51558 [============================>.] - ETA: 0s - loss: 9.0158 - acc: 0.4406Epoch 00000: val_acc did not improve\n",
      "51558/51558 [==============================] - 418s - loss: 9.0157 - acc: 0.4406 - val_loss: 9.4634 - val_acc: 0.4129\n",
      "case2-day4-session2-teacher2 (47885, 1, 3) (47885, 5)\n",
      "(93841, 1, 3) (93841, 5)\n",
      "Train on 47885 samples, validate on 93841 samples\n",
      "Epoch 1/1\n",
      "47884/47885 [============================>.] - ETA: 0s - loss: 11.1407 - acc: 0.3088Epoch 00000: val_acc did not improve\n",
      "47885/47885 [==============================] - 393s - loss: 11.1405 - acc: 0.3088 - val_loss: 9.4634 - val_acc: 0.4129\n",
      "Test score after epoch of whole dataset:\n",
      "[9.4634404389569973, 0.41286857556931406]\n",
      "Test score:\n",
      "[9.4634404389569973, 0.41286857556931406]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Manually create epochs and reset between sessions\n",
    "for i in range(nb_epochs):\n",
    "    # Single epoch. Remember to not shuffle the data!\n",
    "    #print trainX[0:5,:,:]\n",
    "    #print dummy_y_train[0:5,:]\n",
    "    for session in sessions:\n",
    "        sessionX = trainX[numpy.where(times_train.session == session)[0],:,:]\n",
    "        sessionY = dummy_y_train[numpy.where(times_train.session == session)[0],:]\n",
    "        print session, sessionX.shape, sessionY.shape\n",
    "        print testX.shape, dummy_y_test.shape\n",
    "        history = model.fit(sessionX, sessionY, validation_data=(testX,dummy_y_test), \n",
    "                            nb_epoch=1, batch_size=batch_size, shuffle=False, \n",
    "                            verbose=1, callbacks=callbacks_list)\n",
    "        # Remember to reset the state between epochs!\n",
    "        #model.reset_states()\n",
    "    model.reset_states()\n",
    "    # Estimate model performance, and reset states!\n",
    "    testScore = model.evaluate(testX, dummy_y_test, batch_size=batch_size, \n",
    "                                verbose=0)\n",
    "    model.reset_states()\n",
    "    print 'Test score after epoch of whole dataset:'\n",
    "    print testScore\n",
    "\n",
    "# Estimate model performance, and reset states!\n",
    "testScore = model.evaluate(testX, dummy_y_test, batch_size=batch_size, \n",
    "                            verbose=0)\n",
    "model.reset_states()\n",
    "print 'Test score:'\n",
    "print testScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-layer lstm?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_4 (LSTM)                      (1, 1, 20)          1920        lstm_input_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                      (1, 20)             3280        lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                    (1, 5)              105         lstm_5[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 5305\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "# stateful LSTM!\n",
    "model.add(LSTM(20, batch_input_shape=(batch_size, 1, X_train.shape[1]), \n",
    "               stateful=True, return_sequences=True))\n",
    "model.add(LSTM(20, stateful=True))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print model.summary()\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# To save the best model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"acc.model--2lstm.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "filepath=\"acc.weights--2lstm.best.hdf5\"\n",
    "# Define that the accuracy in cv is monitored, and that weights are stored in a file when max accuracy is achieved\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "nb_epochs = 1\n",
    "sessions = numpy.unique(times_train.session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case1-day1-session2-teacher1 (40739, 1, 3) (40739, 5)\n",
      "(93841, 1, 3) (93841, 5)\n",
      "Train on 40739 samples, validate on 93841 samples\n",
      "Epoch 1/1\n",
      "40738/40739 [============================>.] - ETA: 0s - loss: 0.3283 - acc: 0.9088Epoch 00000: val_acc improved from -inf to 0.41287, saving model to acc.weights--2lstm.best.hdf5\n",
      "40739/40739 [==============================] - 431s - loss: 0.3283 - acc: 0.9088 - val_loss: 3.1610 - val_acc: 0.4129\n",
      "case1-day1-session3-teacher1 (38226, 1, 3) (38226, 5)\n",
      "(93841, 1, 3) (93841, 5)\n",
      "Train on 38226 samples, validate on 93841 samples\n",
      "Epoch 1/1\n",
      " 5192/38226 [===>..........................] - ETA: 363s - loss: 6.8705 - acc: 0.5576"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-69b17c2a3856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         history = model.fit(sessionX, sessionY, validation_data=(testX,dummy_y_test), \n\u001b[1;32m     13\u001b[0m                             \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                             verbose=1, callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Remember to reset the state between epochs!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#model.reset_states()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1050\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m                 \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nb_sample'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/utils/generic_utils.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/ipykernel/iostream.pyc\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;31m# wait for flush to actually get through:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mevt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mEvent\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \"\"\"\n\u001b[0;32m--> 549\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_Event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_Event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Verbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0m_Verbose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mCondition\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \"\"\"\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_Condition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_Condition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Verbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lock, verbose)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_acquire_restore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_acquire_restore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Manually create epochs and reset between sessions\n",
    "for i in range(nb_epochs):\n",
    "    # Single epoch. Remember to not shuffle the data!\n",
    "    #print trainX[0:5,:,:]\n",
    "    #print dummy_y_train[0:5,:]\n",
    "    for session in sessions:\n",
    "        sessionX = trainX[numpy.where(times_train.session == session)[0],:,:]\n",
    "        sessionY = dummy_y_train[numpy.where(times_train.session == session)[0],:]\n",
    "        print session, sessionX.shape, sessionY.shape\n",
    "        print testX.shape, dummy_y_test.shape\n",
    "        history = model.fit(sessionX, sessionY, validation_data=(testX,dummy_y_test), \n",
    "                            nb_epoch=1, batch_size=batch_size, shuffle=False, \n",
    "                            verbose=1, callbacks=callbacks_list)\n",
    "        # Remember to reset the state between epochs!\n",
    "        #model.reset_states()\n",
    "    model.reset_states()\n",
    "    # Estimate model performance, and reset states!\n",
    "    testScore = model.evaluate(testX, dummy_y_test, batch_size=batch_size, \n",
    "                                verbose=0)\n",
    "    model.reset_states()\n",
    "    print 'Test score after epoch of whole dataset:'\n",
    "    print testScore\n",
    "\n",
    "# Estimate model performance, and reset states!\n",
    "testScore = model.evaluate(testX, dummy_y_test, batch_size=batch_size, \n",
    "                            verbose=0)\n",
    "model.reset_states()\n",
    "print 'Test score:'\n",
    "print testScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... something may not be working correctly, as all epochs and all LSTM models seem to give the same test score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For comparison, do RF of accel dataset\n",
    "\n",
    "Both on the **raw data**, and on the **10s data** (accel features only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the random forest package\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "# Create the random forest object which will include all the parameters\n",
    "# for the fit\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fit the training data to the Survived labels and create the decision trees\n",
    "forest = forest.fit(train_data[0::,1::],train_data[0::,0])\n",
    "\n",
    "# Take the same decision trees and run it on the test data\n",
    "print 'Accuracy on test data (RAW):',forest.score(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
