{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting from separate data sources - AUDIO\n",
    "\n",
    "... transformed to PCA\n",
    "\n",
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded csvs\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 67\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "data = pandas.read_csv(\"../data/processed/train.csv\")\n",
    "notnull_data = data[data.notnull().all(axis=1)]\n",
    "#train = notnull_data.values\n",
    "data2 = pandas.read_csv(\"../data/processed/test.csv\")\n",
    "notnull_data2 = data2[data2.notnull().all(axis=1)]\n",
    "test = notnull_data2.values\n",
    "\n",
    "# Divide the train dataset further into validation and train sets\n",
    "# keeping in the val set a session from each teacher, \n",
    "# with a variety of tags for Activity and Social\n",
    "#pandas.crosstab(index=data[\"session\"], columns=data[\"Activity\"])\n",
    "#pandas.crosstab(index=data[\"session\"], columns=data[\"Social\"])\n",
    "# e.g., case1-day1-session2-teacher1, case2-day2-session2-teacher2\n",
    "print 'Loaded csvs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(969, 7560)\n",
      "(3503, 7560)\n",
      "Split into train, val, test sets and scaled\n"
     ]
    }
   ],
   "source": [
    "val = notnull_data[notnull_data.session.isin(['case1-day1-session2-teacher1','case2-day2-session2-teacher2'])].values\n",
    "print val.shape\n",
    "tr = notnull_data[~notnull_data.session.isin(['case1-day1-session2-teacher1','case2-day2-session2-teacher2'])].values\n",
    "print tr.shape\n",
    "\n",
    "X_train = tr[:,3:7558].astype(float)\n",
    "Y_trainA = tr[:,7558] #Activity\n",
    "Y_trainS = tr[:,7559] #Social\n",
    "X_val = val[:,3:7558].astype(float)\n",
    "Y_valA = val[:,7558] #Activity\n",
    "Y_valS = val[:,7559] #Social\n",
    "X_test = test[:,3:7558].astype(float)\n",
    "Y_testA = test[:,7558]\n",
    "Y_testS = test[:,7559]\n",
    "\n",
    "# One hot encoding of the response variable (using dummy variables)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# encode class values as integers\n",
    "encoderA = LabelEncoder()\n",
    "encoderA.fit(Y_trainA)\n",
    "\n",
    "encoded_Y_trainA = encoderA.transform(Y_trainA)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_trainA = to_categorical(encoded_Y_trainA)\n",
    "\n",
    "encoded_Y_valA = encoderA.transform(Y_valA)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_valA = to_categorical(encoded_Y_valA)\n",
    "\n",
    "#encoderA.fit(Y_testA)\n",
    "encoded_Y_testA = encoderA.transform(Y_testA)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_testA = to_categorical(encoded_Y_testA)\n",
    "\n",
    "# encode class values as integers\n",
    "encoderS = LabelEncoder()\n",
    "encoderS.fit(Y_trainS)\n",
    "encoded_Y_trainS = encoderS.transform(Y_trainS)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_trainS = to_categorical(encoded_Y_trainS)\n",
    "\n",
    "encoded_Y_valS = encoderS.transform(Y_valS)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_valS = to_categorical(encoded_Y_valS)\n",
    "\n",
    "#encoderS.fit(Y_testS)\n",
    "encoded_Y_testS = encoderS.transform(Y_testS)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_testS = to_categorical(encoded_Y_testS)\n",
    "\n",
    "# We standardize on the basis of the training data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_st = scaler.transform(X_train)\n",
    "X_val_st = scaler.transform(X_val)\n",
    "X_test_st = scaler.transform(X_test)\n",
    "print 'Split into train, val, test sets and scaled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing AUDIO dataset with PCA 100\n",
      "Total variance explained by %d components: 100\n",
      "0.684495816407\n"
     ]
    }
   ],
   "source": [
    "# Reduce the dataset to work with to the AUDIO variables\n",
    "# Get the column names of the X matrix, to separate the different sources\n",
    "colnamesX = list(data.columns.values)[3:7558]\n",
    "# Eyetracking: colnamesX[0:10]\n",
    "# Acc: colnamesX[10:150]\n",
    "# Audio: colnamesX[150:6555]\n",
    "# Video: colnamesX[6555:]\n",
    "X_train_st_audio = X_train_st[:,150:6555]\n",
    "X_val_st_audio = X_val_st[:,150:6555]\n",
    "X_test_st_audio = X_test_st[:,150:6555]\n",
    "# Number of components to extract from the dataset\n",
    "n_components = 100\n",
    "\n",
    "from sklearn import decomposition\n",
    "print 'Reducing AUDIO dataset with PCA',n_components\n",
    "pca = decomposition.PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_st_audio)\n",
    "X_val_pca = pca.transform(X_val_st_audio)\n",
    "X_test_pca = pca.transform(X_test_st_audio)\n",
    "\n",
    "#print 'Variance explained:'\n",
    "#print pca.explained_variance_ratio_\n",
    "print 'Total variance explained by %d components:',n_components\n",
    "print sum(pca.explained_variance_ratio_)\n",
    "\n",
    "trainX = numpy.reshape(X_train_pca, (X_train_pca.shape[0], 1, X_train_pca.shape[1]))\n",
    "valX = numpy.reshape(X_val_pca, (X_val_pca.shape[0], 1, X_val_pca.shape[1]))\n",
    "testX = numpy.reshape(X_test_pca, (X_test_pca.shape[0], 1, X_test_pca.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for training!\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# This is our winning architecture so far\n",
    "def create_LSTM3_PCA(n_outputs, batch_size = 1, trainShape1=100):\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    # stateful LSTM!\n",
    "    model.add(LSTM(200, batch_input_shape=(batch_size, 1, trainShape1), \n",
    "                   return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100, \n",
    "                   return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, \n",
    "                   return_sequences=False, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_LSTM2_PCA(n_outputs, batch_size = 1, trainShape1=100):\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    # stateful LSTM!\n",
    "    model.add(LSTM(300, batch_input_shape=(batch_size, 1, trainShape1), \n",
    "                   return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, \n",
    "                   return_sequences=False, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_LSTM1_PCA(n_outputs, batch_size = 1, trainShape1=100):\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    # stateful LSTM!\n",
    "    model.add(LSTM(400, batch_input_shape=(batch_size, 1, trainShape1), \n",
    "                   return_sequences=False, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, cohen_kappa_score\n",
    "\n",
    "def printValStats(model, testX, dummy_y_test, batch=1):\n",
    "    # Other performance/accuracy metrics\n",
    "    Y_pred = model.predict(testX, batch_size=batch)\n",
    "    model.reset_states()\n",
    "    print 'Performance of model on test set ----------------------------'\n",
    "    # Accuracy\n",
    "    print('Accuracy:')\n",
    "    print(accuracy_score(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1)))\n",
    "    # Kappa\n",
    "    print('Kappa:')\n",
    "    kappa = cohen_kappa_score(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1))\n",
    "    print(kappa)\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1))\n",
    "    numpy.set_printoptions(precision=2)\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    # AUC\n",
    "    roc = roc_auc_score(dummy_y_test, Y_pred, average='macro')\n",
    "    print('AUC score:')\n",
    "    print(roc)\n",
    "    return kappa, roc\n",
    "\n",
    "def plot_training(accs, val_accs, losss, val_losss, kappas, aucs):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(accs)\n",
    "    plt.plot(val_accs)\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(losss)\n",
    "    plt.plot(val_losss)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize kappa and auc\n",
    "    plt.plot(kappas)\n",
    "    plt.plot(aucs)\n",
    "    plt.title('Other performance')\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Kappa','AUC'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "import operator\n",
    "\n",
    "def get_max_values(list):\n",
    "    index, value = max(enumerate(list), key=operator.itemgetter(1))\n",
    "    return index, value\n",
    "\n",
    "print 'Ready for training!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Activity from audio data\n",
    "\n",
    "### 3-layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_1 (LSTM)                    (1, 1, 200)           240800      lstm_input_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (1, 1, 200)           0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (1, 1, 100)           120400      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (1, 1, 100)           0           lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (1, 50)               30200       dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (1, 50)               0           lstm_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (1, 50)               2550        dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (1, 50)               0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (1, 20)               1020        dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (1, 20)               0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (1, 5)                105         dropout_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 395075\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "('Epoch', 1, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 1.2689 - acc: 0.5023Epoch 00000: val_acc improved from -inf to 0.25077, saving model to activity.weights--3lstmaudio.best.hdf5\n",
      "3503/3503 [==============================] - 62s - loss: 1.2693 - acc: 0.5021 - val_loss: 1.9646 - val_acc: 0.2508\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.251805985552\n",
      "Kappa:\n",
      "0.0468951594851\n",
      "Confusion matrix:\n",
      "[[  0   0   0   0 122]\n",
      " [  0 144   0   0 118]\n",
      " [  0 125   0   0 118]\n",
      " [  0  35   0   0 133]\n",
      " [  0  74   0   0 100]]\n",
      "AUC score:\n",
      "0.659596465468\n",
      "('Epoch', 2, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 1.1570 - acc: 0.5485Epoch 00000: val_acc improved from 0.25077 to 0.30857, saving model to activity.weights--3lstmaudio.best.hdf5\n",
      "3503/3503 [==============================] - 70s - loss: 1.1573 - acc: 0.5484 - val_loss: 1.8879 - val_acc: 0.3086\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.310629514964\n",
      "Kappa:\n",
      "0.13378172582\n",
      "Confusion matrix:\n",
      "[[116   1   5   0   0]\n",
      " [ 75 143  44   0   0]\n",
      " [ 69 132  42   0   0]\n",
      " [120  36  12   0   0]\n",
      " [ 45  91  38   0   0]]\n",
      "AUC score:\n",
      "0.61159059271\n",
      "('Epoch', 3, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 1.0809 - acc: 0.5914Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 81s - loss: 1.0812 - acc: 0.5912 - val_loss: 2.0865 - val_acc: 0.2797\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.28689370485\n",
      "Kappa:\n",
      "0.0940494391752\n",
      "Confusion matrix:\n",
      "[[109   0  13   0   0]\n",
      " [ 29 121 112   0   0]\n",
      " [ 74 121  48   0   0]\n",
      " [106  33  29   0   0]\n",
      " [ 28  55  91   0   0]]\n",
      "AUC score:\n",
      "0.586630974546\n",
      "('Epoch', 4, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 1.0058 - acc: 0.6325Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 84s - loss: 1.0058 - acc: 0.6323 - val_loss: 2.0634 - val_acc: 0.3034\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.301341589267\n",
      "Kappa:\n",
      "0.10439465924\n",
      "Confusion matrix:\n",
      "[[101   0  21   0   0]\n",
      " [ 17  97 145   0   3]\n",
      " [ 45 104  94   0   0]\n",
      " [ 99  24  44   0   1]\n",
      " [ 15  28 131   0   0]]\n",
      "AUC score:\n",
      "0.595045966039\n",
      "('Epoch', 5, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.9295 - acc: 0.6710Epoch 00000: val_acc improved from 0.30857 to 0.32611, saving model to activity.weights--3lstmaudio.best.hdf5\n",
      "3503/3503 [==============================] - 85s - loss: 0.9295 - acc: 0.6709 - val_loss: 1.8770 - val_acc: 0.3261\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.334365325077\n",
      "Kappa:\n",
      "0.134203002144\n",
      "Confusion matrix:\n",
      "[[ 93   0  23   0   6]\n",
      " [ 12 124  98   0  28]\n",
      " [ 16 131  95   0   1]\n",
      " [ 47  31  87   0   3]\n",
      " [ 12  74  76   0  12]]\n",
      "AUC score:\n",
      "0.671039582073\n",
      "('Epoch', 6, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.8512 - acc: 0.7013Epoch 00000: val_acc improved from 0.32611 to 0.35913, saving model to activity.weights--3lstmaudio.best.hdf5\n",
      "3503/3503 [==============================] - 90s - loss: 0.8511 - acc: 0.7014 - val_loss: 2.1873 - val_acc: 0.3591\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.37048503612\n",
      "Kappa:\n",
      "0.195028183342\n",
      "Confusion matrix:\n",
      "[[105   3  10   0   4]\n",
      " [ 25  41 191   0   5]\n",
      " [ 33  11 199   0   0]\n",
      " [ 78  17  73   0   0]\n",
      " [ 19  46  95   0  14]]\n",
      "AUC score:\n",
      "0.636614370532\n",
      "('Epoch', 7, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.7602 - acc: 0.7479Epoch 00000: val_acc improved from 0.35913 to 0.37771, saving model to activity.weights--3lstmaudio.best.hdf5\n",
      "3503/3503 [==============================] - 92s - loss: 0.7601 - acc: 0.7479 - val_loss: 1.9019 - val_acc: 0.3777\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.37048503612\n",
      "Kappa:\n",
      "0.186228848269\n",
      "Confusion matrix:\n",
      "[[ 84   1  31   6   0]\n",
      " [  4  64 146  23  25]\n",
      " [ 13  48 172   6   4]\n",
      " [ 42  29  83   6   8]\n",
      " [  6  59  66  10  33]]\n",
      "AUC score:\n",
      "0.702416028237\n",
      "('Epoch', 8, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.7144 - acc: 0.7676Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 94s - loss: 0.7143 - acc: 0.7676 - val_loss: 1.9181 - val_acc: 0.3560\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.355005159959\n",
      "Kappa:\n",
      "0.161198151831\n",
      "Confusion matrix:\n",
      "[[ 83   2  36   0   1]\n",
      " [  4  62 178   4  14]\n",
      " [ 12  57 169   0   5]\n",
      " [ 47  31  82   2   6]\n",
      " [  6  73  64   3  28]]\n",
      "AUC score:\n",
      "0.675981713055\n",
      "('Epoch', 9, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.6623 - acc: 0.7864Epoch 00000: val_acc improved from 0.37771 to 0.38803, saving model to activity.weights--3lstmaudio.best.hdf5\n",
      "3503/3503 [==============================] - 92s - loss: 0.6621 - acc: 0.7865 - val_loss: 2.2118 - val_acc: 0.3880\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.391124871001\n",
      "Kappa:\n",
      "0.217428441195\n",
      "Confusion matrix:\n",
      "[[ 94   2  22   0   4]\n",
      " [  8  26 197   6  25]\n",
      " [ 14   6 217   0   6]\n",
      " [ 54   9  89   7   9]\n",
      " [  7  18 109   5  35]]\n",
      "AUC score:\n",
      "0.679616741276\n",
      "('Epoch', 10, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.6179 - acc: 0.7975Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 93s - loss: 0.6185 - acc: 0.7973 - val_loss: 2.1570 - val_acc: 0.3860\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.418988648091\n",
      "Kappa:\n",
      "0.253274768474\n",
      "Confusion matrix:\n",
      "[[ 85  13  21   2   1]\n",
      " [ 11 118  80   4  49]\n",
      " [ 14  79 132   3  15]\n",
      " [ 46  20  62  15  25]\n",
      " [ 10  42  62   4  56]]\n",
      "AUC score:\n",
      "0.701809581521\n",
      "('Epoch', 11, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.5884 - acc: 0.8067Epoch 00000: val_acc improved from 0.38803 to 0.46233, saving model to activity.weights--3lstmaudio.best.hdf5\n",
      "3503/3503 [==============================] - 89s - loss: 0.5883 - acc: 0.8067 - val_loss: 1.8456 - val_acc: 0.4623\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.468524251806\n",
      "Kappa:\n",
      "0.323859448503\n",
      "Confusion matrix:\n",
      "[[ 82  11  18   4   7]\n",
      " [ 13 101  71  19  58]\n",
      " [ 15  41 161   6  20]\n",
      " [ 50  31  35  29  23]\n",
      " [  8  37  34  14  81]]\n",
      "AUC score:\n",
      "0.727081838647\n",
      "('Epoch', 12, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.5101 - acc: 0.8452Epoch 00000: val_acc improved from 0.46233 to 0.47472, saving model to activity.weights--3lstmaudio.best.hdf5\n",
      "3503/3503 [==============================] - 88s - loss: 0.5100 - acc: 0.8453 - val_loss: 1.9867 - val_acc: 0.4747\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.479876160991\n",
      "Kappa:\n",
      "0.338037599794\n",
      "Confusion matrix:\n",
      "[[ 97   6  15   1   3]\n",
      " [ 20 108  80   6  48]\n",
      " [ 20  25 186   1  11]\n",
      " [ 62  29  26  18  33]\n",
      " [ 16  64  34   4  56]]\n",
      "AUC score:\n",
      "0.736110166754\n",
      "('Epoch', 13, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.4865 - acc: 0.8555Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 87s - loss: 0.4864 - acc: 0.8556 - val_loss: 2.2015 - val_acc: 0.4231\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.422084623323\n",
      "Kappa:\n",
      "0.264623774237\n",
      "Confusion matrix:\n",
      "[[ 88   5  25   1   3]\n",
      " [ 22  78  99  16  47]\n",
      " [ 21  23 177   7  15]\n",
      " [ 54  23  54   8  29]\n",
      " [ 12  40  52  12  58]]\n",
      "AUC score:\n",
      "0.714565790409\n",
      "('Epoch', 14, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.4324 - acc: 0.8675Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 90s - loss: 0.4323 - acc: 0.8675 - val_loss: 2.2272 - val_acc: 0.4303\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.43137254902\n",
      "Kappa:\n",
      "0.277056605306\n",
      "Confusion matrix:\n",
      "[[ 96   7  10   3   6]\n",
      " [  8  74 113  11  56]\n",
      " [ 24  16 183   9  11]\n",
      " [ 66  23  33  17  29]\n",
      " [  7  50  61   8  48]]\n",
      "AUC score:\n",
      "0.715783232956\n",
      "('Epoch', 15, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.4402 - acc: 0.8601Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 92s - loss: 0.4401 - acc: 0.8601 - val_loss: 2.3535 - val_acc: 0.3953\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.389060887513\n",
      "Kappa:\n",
      "0.229362382973\n",
      "Confusion matrix:\n",
      "[[ 83   1  26   4   8]\n",
      " [ 14  31 144  16  57]\n",
      " [ 21  12 178  20  12]\n",
      " [ 69   8  34  25  32]\n",
      " [ 13  19  62  20  60]]\n",
      "AUC score:\n",
      "0.711972590191\n",
      "('Epoch', 16, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.4029 - acc: 0.8789Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 97s - loss: 0.4028 - acc: 0.8790 - val_loss: 2.0671 - val_acc: 0.4685\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.463364293086\n",
      "Kappa:\n",
      "0.323050384301\n",
      "Confusion matrix:\n",
      "[[ 92   6  11   4   9]\n",
      " [ 12 127  28  16  79]\n",
      " [ 22  68 122  11  20]\n",
      " [ 54  20   6  29  59]\n",
      " [ 18  52  17   8  79]]\n",
      "AUC score:\n",
      "0.741876928989\n",
      "('Epoch', 17, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.3576 - acc: 0.8992Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 88s - loss: 0.3575 - acc: 0.8992 - val_loss: 2.1797 - val_acc: 0.4737\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.488132094943\n",
      "Kappa:\n",
      "0.352124295339\n",
      "Confusion matrix:\n",
      "[[ 86   9  12  12   3]\n",
      " [  7  98  76   6  75]\n",
      " [ 18  18 173  13  21]\n",
      " [ 54  21  20  42  31]\n",
      " [ 12  43  32  13  74]]\n",
      "AUC score:\n",
      "0.749655365107\n",
      "('Epoch', 18, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.3204 - acc: 0.9118Epoch 00000: val_acc improved from 0.47472 to 0.48400, saving model to activity.weights--3lstmaudio.best.hdf5\n",
      "3503/3503 [==============================] - 80s - loss: 0.3204 - acc: 0.9118 - val_loss: 2.1297 - val_acc: 0.4840\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.486068111455\n",
      "Kappa:\n",
      "0.351600836294\n",
      "Confusion matrix:\n",
      "[[ 95   5   5   8   9]\n",
      " [ 10  93  80  13  66]\n",
      " [ 19  26 170  16  12]\n",
      " [ 55  23   6  39  45]\n",
      " [ 16  46  26  12  74]]\n",
      "AUC score:\n",
      "0.747603640693\n",
      "('Epoch', 19, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.3161 - acc: 0.9121Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 76s - loss: 0.3160 - acc: 0.9121 - val_loss: 2.3955 - val_acc: 0.4696\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.470588235294\n",
      "Kappa:\n",
      "0.332975956932\n",
      "Confusion matrix:\n",
      "[[ 89   1  11  12   9]\n",
      " [  4  62 105  31  60]\n",
      " [ 19  17 171  15  21]\n",
      " [ 41  13  11  58  45]\n",
      " [  8  34  37  19  76]]\n",
      "AUC score:\n",
      "0.740531586247\n",
      "('Epoch', 20, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.9132Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 86s - loss: 0.3217 - acc: 0.9132 - val_loss: 2.5254 - val_acc: 0.4087\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.410732714138\n",
      "Kappa:\n",
      "0.252878172877\n",
      "Confusion matrix:\n",
      "[[ 93   3  18   4   4]\n",
      " [ 12  64 120  11  55]\n",
      " [ 24  37 163   4  15]\n",
      " [ 57  22  38   7  44]\n",
      " [ 15  27  52   9  71]]\n",
      "AUC score:\n",
      "0.691995854452\n",
      "('Epoch', 21, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.9163Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 85s - loss: 0.2961 - acc: 0.9164 - val_loss: 2.4601 - val_acc: 0.4293\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.434468524252\n",
      "Kappa:\n",
      "0.285308784043\n",
      "Confusion matrix:\n",
      "[[ 89   7   8  15   3]\n",
      " [  5 121  49  29  58]\n",
      " [ 18  94  78  43  10]\n",
      " [ 46  28  13  60  21]\n",
      " [ 17  36  30  18  73]]\n",
      "AUC score:\n",
      "0.703658205573\n",
      "('Epoch', 22, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.2719 - acc: 0.9243Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 90s - loss: 0.2718 - acc: 0.9244 - val_loss: 2.4873 - val_acc: 0.4613\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.458204334365\n",
      "Kappa:\n",
      "0.316805595285\n",
      "Confusion matrix:\n",
      "[[100   4  12   3   3]\n",
      " [ 15  75  87  24  61]\n",
      " [ 18  21 177  14  13]\n",
      " [ 57  17  38  27  29]\n",
      " [ 25  32  38  14  65]]\n",
      "AUC score:\n",
      "0.730885216947\n",
      "('Epoch', 23, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.3155 - acc: 0.9058Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 84s - loss: 0.3154 - acc: 0.9058 - val_loss: 2.5323 - val_acc: 0.4417\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.433436532508\n",
      "Kappa:\n",
      "0.283551395576\n",
      "Confusion matrix:\n",
      "[[ 91   2  11  14   4]\n",
      " [  4  79  89  27  63]\n",
      " [ 13  26 158  24  22]\n",
      " [ 58  17  36  36  21]\n",
      " [  8  34  50  26  56]]\n",
      "AUC score:\n",
      "0.70685528936\n",
      "('Epoch', 24, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.2954 - acc: 0.9218Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 75s - loss: 0.2954 - acc: 0.9218 - val_loss: 2.6807 - val_acc: 0.4159\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.412796697626\n",
      "Kappa:\n",
      "0.261869954617\n",
      "Confusion matrix:\n",
      "[[ 88   5   9  17   3]\n",
      " [ 13  79  72  39  59]\n",
      " [ 16  49 116  42  20]\n",
      " [ 60  23  18  42  25]\n",
      " [  9  21  52  17  75]]\n",
      "AUC score:\n",
      "0.691513256187\n",
      "('Epoch', 25, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.9249Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 77s - loss: 0.2528 - acc: 0.9249 - val_loss: 2.5991 - val_acc: 0.4644\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.457172342621\n",
      "Kappa:\n",
      "0.305813421371\n",
      "Confusion matrix:\n",
      "[[ 73   6  17  21   5]\n",
      " [  3  96  98  20  45]\n",
      " [  8  18 176  23  18]\n",
      " [ 37  30  34  53  14]\n",
      " [  4  50  50  25  45]]\n",
      "AUC score:\n",
      "0.717191200676\n",
      "('Epoch', 26, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "3502/3503 [============================>.] - ETA: 0s - loss: 0.2623 - acc: 0.9300Epoch 00000: val_acc did not improve\n",
      "3503/3503 [==============================] - 79s - loss: 0.2622 - acc: 0.9301 - val_loss: 2.4574 - val_acc: 0.4623\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.460268317853\n",
      "Kappa:\n",
      "0.31829224329\n",
      "Confusion matrix:\n",
      "[[ 81   3  15  16   7]\n",
      " [  4  78  69  58  53]\n",
      " [ 12  29 171  18  13]\n",
      " [ 44  17  36  43  28]\n",
      " [ 12  18  44  27  73]]\n",
      "AUC score:\n",
      "0.734716951012\n",
      "('Epoch', 27, '/', 100)\n",
      "Train on 3503 samples, validate on 969 samples\n",
      "Epoch 1/1\n",
      "1994/3503 [================>.............] - ETA: 31s - loss: 0.2471 - acc: 0.9303"
     ]
    }
   ],
   "source": [
    "# Create the model and parameters for training\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "batch = 1\n",
    "epochs = 100\n",
    "\n",
    "modelA3 = create_LSTM3_PCA(dummy_y_trainA.shape[1], batch_size = batch, trainShape1=n_components)\n",
    "print modelA3.summary()\n",
    "\n",
    "# To save the best model\n",
    "# serialize model to JSON\n",
    "modelA3_json = modelA3.to_json()\n",
    "with open(\"activity.model--3lstmaudio.json\", \"w\") as json_file:\n",
    "    json_file.write(modelA3_json)\n",
    "filepathA3=\"activity.weights--3lstmaudio.best.hdf5\"\n",
    "# Define that the accuracy in cv is monitored, and that weights are stored in a file when max accuracy is achieved\n",
    "checkpointA3 = ModelCheckpoint(filepathA3, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_listA3 = [checkpointA3]\n",
    "\n",
    "# Fit the model\n",
    "accs =[]\n",
    "val_accs =[]\n",
    "losss =[]\n",
    "val_losss =[]\n",
    "kappas = []\n",
    "aucs = []\n",
    "\n",
    "# Manually create epochs and reset between sessions\n",
    "for i in range(epochs):\n",
    "    # Single epoch. Remember to not shuffle the data!\n",
    "    print('Epoch', i+1, '/', epochs)\n",
    "    history = modelA3.fit(trainX, dummy_y_trainA, validation_data=(valX, dummy_y_valA), \n",
    "                        nb_epoch=1, batch_size=batch, shuffle=False, \n",
    "                        verbose=1, callbacks=callbacks_listA3)\n",
    "    modelA3.reset_states()\n",
    "    kappa, auc = printValStats(modelA3, valX, dummy_y_valA, batch=batch)\n",
    "    accs.append(history.history['acc'][0])\n",
    "    val_accs.append(history.history['val_acc'][0])\n",
    "    losss.append(history.history['loss'][0])\n",
    "    val_losss.append(history.history['val_loss'][0])\n",
    "    kappas.append(kappa)\n",
    "    aucs.append(auc)\n",
    "    \n",
    "print 'Best validation accuracy: ', get_max_values(val_accs)\n",
    "plot_training(accs, val_accs, losss, val_losss, kappas, aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline + HMM states in the mix\n",
    "see https://github.com/hmmlearn/hmmlearn and https://github.com/larsmans/seqlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and evaluating the best models\n",
    "\n",
    "## 3-layer LSTM (Activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "# LOAD AND USE MODEL\n",
    "json_file3 = open('activity.model--3lstmbis.json','r')\n",
    "loaded_model_json3 = json_file3.read()\n",
    "json_file3.close()\n",
    "loaded_model3 = model_from_json(loaded_model_json3)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model3.load_weights(\"activity.weights--3lstmbis.best.hdf5\")\n",
    "print(\"Loaded model 3 from disk\")\n",
    "# evaluate loaded model on test data\n",
    "# IMPORTANT: compile the model again before use!\n",
    "loaded_model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "score3 = loaded_model3.evaluate(testX, dummy_y_testA, batch_size=1, verbose=0)\n",
    "print \"3 Layer LSTM --- %s: %.2f%%\" % (loaded_model3.metrics_names[1], score3[1]*100)\n",
    "printValStats(loaded_model3,  testX, dummy_y_testA, batch=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
