{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Activity with an LSTM\n",
    "i.e., full dataset, transformed to PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 66\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "data = pandas.read_csv(\"../data/processed/train.csv\")\n",
    "notnull_data = data[data.notnull().all(axis=1)]\n",
    "train = notnull_data.values\n",
    "data2 = pandas.read_csv(\"../data/processed/test.csv\")\n",
    "notnull_data2 = data2[data2.notnull().all(axis=1)]\n",
    "test = notnull_data2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4472, 7555)\n",
      "(1044,)\n",
      "(1044, 5)\n"
     ]
    }
   ],
   "source": [
    "X_train = train[:,3:7558].astype(float)\n",
    "#X_train = train[:,3:13].astype(float)\n",
    "Y_train = train[:,7558]\n",
    "X_test = test[:,3:7558].astype(float)\n",
    "#X_test = test[:,3:13].astype(float)\n",
    "Y_test = test[:,7558]\n",
    "\n",
    "# One hot encoding of the response variable (using dummy variables)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_train)\n",
    "encoded_Y_train = encoder.transform(Y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_train = to_categorical(encoded_Y_train)\n",
    "encoder.fit(Y_test)\n",
    "encoded_Y_test = encoder.transform(Y_test)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_test = to_categorical(encoded_Y_test)\n",
    "\n",
    "# Sanity check on matrix dimensions, after droppinig null/nans\n",
    "print X_train.shape #(4472, 7555)\n",
    "print Y_test.shape #(1044, )\n",
    "print dummy_y_test.shape # (1044, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained:\n",
      "[ 0.19505564  0.08295663  0.04067402  0.02174272  0.01775119  0.01686748\n",
      "  0.01440615  0.01150575  0.01055     0.00958828  0.00877439  0.00790115\n",
      "  0.007298    0.00704889  0.0066142   0.00595329  0.00538577  0.00530285\n",
      "  0.00491633  0.0045751   0.00440203  0.00436509  0.00395662  0.00343414\n",
      "  0.00340722  0.00328142  0.00325585  0.00305899  0.00297009  0.00289683\n",
      "  0.00281987  0.00273652  0.00266189  0.00265268  0.00260069  0.0024912\n",
      "  0.00238337  0.00232648  0.00226495  0.00224082  0.00221907  0.00217391\n",
      "  0.00215321  0.00208428  0.00204535  0.00198475  0.00192726  0.00190714\n",
      "  0.00187787  0.00183914  0.00176811  0.00173735  0.00169542  0.00166308\n",
      "  0.00165403  0.00162881  0.00161277  0.0015787   0.00157364  0.00155227\n",
      "  0.0015123   0.00149719  0.00148685  0.00147954  0.00145035  0.00142099\n",
      "  0.00141436  0.00141164  0.00138952  0.00136064  0.00134219  0.0013185\n",
      "  0.00130973  0.00129871  0.00129763  0.0012655   0.00125215  0.00124378\n",
      "  0.00122832  0.0012048   0.00119168  0.00117427  0.0011715   0.00116194\n",
      "  0.0011515   0.00113744  0.00112647  0.00111649  0.00111051  0.00109261\n",
      "  0.00108877  0.00107673  0.00107208  0.00105653  0.00104749  0.00103841\n",
      "  0.00103485  0.00101975  0.00100922  0.00100201]\n",
      "Total variance explained by 100 components:\n",
      "0.630815671837\n"
     ]
    }
   ],
   "source": [
    "# We standardize on the basis of the training data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_st = scaler.transform(X_train)\n",
    "X_test_st = scaler.transform(X_test)\n",
    "\n",
    "from sklearn import decomposition\n",
    "\n",
    "pca = decomposition.PCA(n_components=100)\n",
    "X_train_pca = pca.fit_transform(X_train_st)\n",
    "X_test_pca = pca.transform(X_test_st)\n",
    "\n",
    "print 'Variance explained:'\n",
    "print pca.explained_variance_ratio_\n",
    "print 'Total variance explained by 100 components:'\n",
    "print sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                       Output Shape        Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_4 (LSTM)                      (1, 1, 60)          38640       lstm_input_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)                (1, 1, 60)          0           lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                      (1, 30)             10920       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)                (1, 30)             0           lstm_5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                    (1, 5)              155         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 49715\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "(4472, 1, 100)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# baseline model\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=7555, init='uniform', activation='tanh', W_constraint=maxnorm(4)))\n",
    "    model.add(Dense(20, init='uniform', activation='tanh', W_constraint=maxnorm(4)))\n",
    "    model.add(Dense(5, init='uniform', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Apply dropout regularization, it is overfitting!\n",
    "def create_dropout():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(7555,)))\n",
    "    model.add(Dense(200, init='uniform', activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, init='uniform', activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, init='uniform', activation='sigmoid'))\n",
    "    # Compile model, with larger learning rate and momentum, as recommended by the original paper\n",
    "    sgd = SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Apply dropout regularization, it is overfitting!\n",
    "def create_dropout_decay():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(7555,)))\n",
    "    model.add(Dense(200, init='uniform', activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, init='uniform', activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, init='uniform', activation='sigmoid'))\n",
    "    # Compile model, with larger learning rate and momentum, as recommended by the original paper\n",
    "    sgd = SGD(lr=0.1, momentum=0.9, decay=0.005, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Apply dropout regularization, it is overfitting!\n",
    "def create_deeper_dropout_decay_PCA():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(100,)))\n",
    "    model.add(Dense(300, init='uniform', activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(300, init='uniform', activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(80, init='uniform', activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(80, init='uniform', activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, init='uniform', activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, init='uniform', activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, init='uniform', activation='sigmoid'))\n",
    "    # Compile model, with larger learning rate and momentum, as recommended by the original paper\n",
    "    sgd = SGD(lr=0.1, momentum=0.8, decay=0.0001, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_LSTM_PCA(batch_size = 1, trainShape1=100):\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    # stateful LSTM!\n",
    "    model.add(LSTM(20, batch_input_shape=(batch_size, 1, trainShape1), \n",
    "                   stateful=True))\n",
    "    model.add(Dense(5, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_LSTM2_PCA(batch_size = 1, trainShape1=100):\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    # stateful LSTM!\n",
    "    model.add(LSTM(60, batch_input_shape=(batch_size, 1, trainShape1), \n",
    "                   return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(30, batch_input_shape=(batch_size, 1, trainShape1), \n",
    "                   return_sequences=False, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# evaluate baseline model with standardized dataset\n",
    "numpy.random.seed(seed)\n",
    "#estimators = []\n",
    "#estimators.append(('standardize', StandardScaler()))\n",
    "#estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, nb_epoch=10, batch_size=10, verbose=1)))\n",
    "# We define a pipeline of estimators, in which first the scaler is fitted to the data, then the MLP is applied\n",
    "#pipeline = Pipeline(estimators)\n",
    "#kfold = StratifiedKFold(y=Y_train, n_folds=3, shuffle=True, random_state=seed)\n",
    "\n",
    "#model = create_baseline()\n",
    "model = create_LSTM2_PCA()\n",
    "print model.summary()\n",
    "\n",
    "trainX = numpy.reshape(X_train_pca, (X_train_pca.shape[0], 1, X_train_pca.shape[1]))\n",
    "testX = numpy.reshape(X_test_pca, (X_test_pca.shape[0], 1, X_test_pca.shape[1]))\n",
    "print trainX.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', 1, '/', 100)\n",
      "Train on 4472 samples, validate on 1044 samples\n",
      "Epoch 1/1\n",
      "4471/4472 [============================>.] - ETA: 0s - loss: 0.8149 - acc: 0.6860Epoch 00000: val_acc improved from -inf to 0.59195, saving model to weights--2lstm.best.hdf5\n",
      "4472/4472 [==============================] - 50s - loss: 0.8148 - acc: 0.6860 - val_loss: 1.0607 - val_acc: 0.5920\n",
      "Test score after epoch of whole dataset:\n",
      "[1.0619434994909469, 0.59195402298850575]\n",
      "('Epoch', 2, '/', 100)\n",
      "Train on 4472 samples, validate on 1044 samples\n",
      "Epoch 1/1\n",
      "3402/4472 [=====================>........] - ETA: 13s - loss: 0.7847 - acc: 0.7016"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# To save the best model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model--2lstm.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "filepath=\"weights--2lstm.best.hdf5\"\n",
    "# Define that the accuracy in cv is monitored, and that weights are stored in a file when max accuracy is achieved\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Fit the model\n",
    "batch_size = 1\n",
    "nb_epochs = 100\n",
    "accs =[]\n",
    "val_accs =[]\n",
    "losss =[]\n",
    "val_losss =[]\n",
    "\n",
    "# Manually create epochs and reset between sessions\n",
    "for i in range(nb_epochs):\n",
    "    # Single epoch. Remember to not shuffle the data!\n",
    "    print('Epoch', i+1, '/', nb_epochs)\n",
    "    #print trainX[0:5,:,:]\n",
    "    #print dummy_y_train[0:5,:]\n",
    "    history = model.fit(trainX, dummy_y_train, validation_data=(testX,dummy_y_test), \n",
    "                        nb_epoch=1, batch_size=batch_size, shuffle=False, \n",
    "                        verbose=1, callbacks=callbacks_list)\n",
    "    #accs = accs.append(history.history['acc'])\n",
    "    #val_accs = val_accs.append(history.history['val_acc'])\n",
    "    #losss = losss.append(history.history['loss'])\n",
    "    #val_losss = val_losss.append(history.history['val_loss'])\n",
    "\n",
    "    model.reset_states()\n",
    "    # Estimate model performance, and reset states!\n",
    "    testScore = model.evaluate(testX, dummy_y_test, batch_size=batch_size, \n",
    "                                verbose=0)\n",
    "    model.reset_states()\n",
    "    print 'Test score after epoch of whole dataset:'\n",
    "    print testScore\n",
    "\n",
    "# Estimate model performance, and reset states!\n",
    "testScore = model.evaluate(testX, dummy_y_test, batch_size=batch_size, \n",
    "                            verbose=0)\n",
    "model.reset_states()\n",
    "print 'Final Test score:'\n",
    "print testScore\n",
    "\n",
    "#results = cross_val_score(pipeline, X_train, dummy_y_train, cv=kfold)\n",
    "#print(\"Standardized data Acc (in CV training data): %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "# evaluate the model\n",
    "#scores = pipeline.evaluate(X_test, dummy_y_test)\n",
    "#print pipeline.metrics_names[1]\n",
    "#print scores[1]*100\n",
    "# For other metrics, see http://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score\n",
    "\n",
    "# Other performance/accuracy metrics\n",
    "Y_pred = model.predict(testX, batch_size=batch_size)\n",
    "model.reset_states()\n",
    "print Y_pred.shape\n",
    "\n",
    "# Accuracy\n",
    "print('Accuracy:')\n",
    "print(accuracy_score(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1)))\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1))\n",
    "numpy.set_printoptions(precision=2)\n",
    "print('Confusion matrix:')\n",
    "print(cm)\n",
    "\n",
    "# AUC\n",
    "roc = roc_auc_score(dummy_y_test, Y_pred, average='macro')\n",
    "print('AUC score:')\n",
    "print(roc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
