{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting with LSTMs -- Second round\n",
    "i.e., full dataset, transformed to PCA\n",
    "\n",
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 67\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "data = pandas.read_csv(\"../data/processed/train.csv\")\n",
    "notnull_data = data[data.notnull().all(axis=1)]\n",
    "train = notnull_data.values\n",
    "data2 = pandas.read_csv(\"../data/processed/test.csv\")\n",
    "notnull_data2 = data2[data2.notnull().all(axis=1)]\n",
    "test = notnull_data2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing dataset with PCA 100\n",
      "Total variance explained by %d components: 100\n",
      "0.630815671837\n"
     ]
    }
   ],
   "source": [
    "X_train = train[:,3:7558].astype(float)\n",
    "Y_trainA = train[:,7558] #Activity\n",
    "Y_trainS = train[:,7559] #Social\n",
    "X_test = test[:,3:7558].astype(float)\n",
    "Y_testA = test[:,7558]\n",
    "Y_testS = test[:,7559]\n",
    "\n",
    "# One hot encoding of the response variable (using dummy variables)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# encode class values as integers\n",
    "encoderA = LabelEncoder()\n",
    "encoderA.fit(Y_trainA)\n",
    "encoded_Y_trainA = encoderA.transform(Y_trainA)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_trainA = to_categorical(encoded_Y_trainA)\n",
    "encoderA.fit(Y_testA)\n",
    "encoded_Y_testA = encoderA.transform(Y_testA)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_testA = to_categorical(encoded_Y_testA)\n",
    "\n",
    "# encode class values as integers\n",
    "encoderS = LabelEncoder()\n",
    "encoderS.fit(Y_trainS)\n",
    "encoded_Y_trainS = encoderS.transform(Y_trainS)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_trainS = to_categorical(encoded_Y_trainS)\n",
    "encoderS.fit(Y_testS)\n",
    "encoded_Y_testS = encoderS.transform(Y_testS)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y_testS = to_categorical(encoded_Y_testS)\n",
    "\n",
    "# We standardize on the basis of the training data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_st = scaler.transform(X_train)\n",
    "X_test_st = scaler.transform(X_test)\n",
    "\n",
    "# Number of components to extract from the dataset\n",
    "n_components = 100\n",
    "\n",
    "from sklearn import decomposition\n",
    "print 'Reducing dataset with PCA',n_components\n",
    "pca = decomposition.PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_st)\n",
    "X_test_pca = pca.transform(X_test_st)\n",
    "\n",
    "#print 'Variance explained:'\n",
    "#print pca.explained_variance_ratio_\n",
    "print 'Total variance explained by %d components:',n_components\n",
    "print sum(pca.explained_variance_ratio_)\n",
    "\n",
    "trainX = numpy.reshape(X_train_pca, (X_train_pca.shape[0], 1, X_train_pca.shape[1]))\n",
    "testX = numpy.reshape(X_test_pca, (X_test_pca.shape[0], 1, X_test_pca.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for training!\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# This is our winning architecture so far\n",
    "def create_LSTM3_PCA(n_outputs, batch_size = 1, trainShape1=100):\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    # stateful LSTM!\n",
    "    model.add(LSTM(200, batch_input_shape=(batch_size, 1, trainShape1), \n",
    "                   return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100, \n",
    "                   return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, \n",
    "                   return_sequences=False, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_LSTM2_PCA(n_outputs, batch_size = 1, trainShape1=100):\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    # stateful LSTM!\n",
    "    model.add(LSTM(300, batch_input_shape=(batch_size, 1, trainShape1), \n",
    "                   return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, \n",
    "                   return_sequences=False, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_LSTM1_PCA(n_outputs, batch_size = 1, trainShape1=100):\n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    # stateful LSTM!\n",
    "    model.add(LSTM(400, batch_input_shape=(batch_size, 1, trainShape1), \n",
    "                   return_sequences=False, stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(20, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, cohen_kappa_score\n",
    "\n",
    "def printValStats(model, testX, dummy_y_test, batch=1):\n",
    "    # Other performance/accuracy metrics\n",
    "    Y_pred = model.predict(testX, batch_size=batch)\n",
    "    model.reset_states()\n",
    "    print 'Performance of model on test set ----------------------------'\n",
    "    # Accuracy\n",
    "    print('Accuracy:')\n",
    "    print(accuracy_score(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1)))\n",
    "    # Kappa\n",
    "    print('Kappa:')\n",
    "    kappa = cohen_kappa_score(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1))\n",
    "    print(kappa)\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(numpy.argmax(dummy_y_test, axis=1), numpy.argmax(Y_pred, axis=1))\n",
    "    numpy.set_printoptions(precision=2)\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    # AUC\n",
    "    roc = roc_auc_score(dummy_y_test, Y_pred, average='macro')\n",
    "    print('AUC score:')\n",
    "    print(roc)\n",
    "    return kappa, roc\n",
    "\n",
    "def plot_training(accs, val_accs, losss, val_losss, kappas, aucs):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(accs)\n",
    "    plt.plot(val_accs)\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(losss)\n",
    "    plt.plot(val_losss)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize kappa and auc\n",
    "    plt.plot(kappas)\n",
    "    plt.plot(aucs)\n",
    "    plt.title('Other performance')\n",
    "    plt.ylabel('metric')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Kappa','AUC'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "import operator\n",
    "\n",
    "def get_max_values(list):\n",
    "    index, value = max(enumerate(list), key=operator.itemgetter(1))\n",
    "    return index, value\n",
    "\n",
    "print 'Ready for training!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Activity\n",
    "\n",
    "### 3-layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the model and parameters for training\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "batch = 1\n",
    "epochs = 100\n",
    "\n",
    "modelA3 = create_LSTM3_PCA(dummy_y_trainA.shape[1], batch_size = batch, trainShape1=n_components)\n",
    "print modelA3.summary()\n",
    "\n",
    "# To save the best model\n",
    "# serialize model to JSON\n",
    "modelA3_json = modelA3.to_json()\n",
    "with open(\"activity.model--3lstmbis.json\", \"w\") as json_file:\n",
    "    json_file.write(modelA3_json)\n",
    "filepathA3=\"activity.weights--3lstmbis.best.hdf5\"\n",
    "# Define that the accuracy in cv is monitored, and that weights are stored in a file when max accuracy is achieved\n",
    "checkpointA3 = ModelCheckpoint(filepathA3, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_listA3 = [checkpointA3]\n",
    "\n",
    "# Fit the model\n",
    "accs =[]\n",
    "val_accs =[]\n",
    "losss =[]\n",
    "val_losss =[]\n",
    "kappas = []\n",
    "aucs = []\n",
    "\n",
    "# Manually create epochs and reset between sessions\n",
    "for i in range(epochs):\n",
    "    # Single epoch. Remember to not shuffle the data!\n",
    "    print('Epoch', i+1, '/', epochs)\n",
    "    history = modelA3.fit(trainX, dummy_y_trainA, validation_data=(testX, dummy_y_testA), \n",
    "                        nb_epoch=1, batch_size=batch, shuffle=False, \n",
    "                        verbose=1, callbacks=callbacks_listA3)\n",
    "    modelA3.reset_states()\n",
    "    kappa, auc = printValStats(modelA3, testX, dummy_y_testA, batch=batch)\n",
    "    accs.append(history.history['acc'][0])\n",
    "    val_accs.append(history.history['val_acc'][0])\n",
    "    losss.append(history.history['loss'][0])\n",
    "    val_losss.append(history.history['val_loss'][0])\n",
    "    kappas.append(kappa)\n",
    "    aucs.append(auc)\n",
    "    \n",
    "print 'Best validation accuracy: ', get_max_values(val_accs)\n",
    "plot_training(accs, val_accs, losss, val_losss, kappas, aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-layer (wider) LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the model and parameters for training\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "batch = 1\n",
    "epochs = 100\n",
    "\n",
    "modelA2 = create_LSTM2_PCA(dummy_y_trainA.shape[1], batch_size = batch, trainShape1=n_components)\n",
    "print modelA2.summary()\n",
    "\n",
    "# To save the best model\n",
    "# serialize model to JSON\n",
    "modelA2_json = modelA2.to_json()\n",
    "with open(\"activity.model--2lstmbis.json\", \"w\") as json_file:\n",
    "    json_file.write(modelA2_json)\n",
    "filepathA2=\"activity.weights--2lstmbis.best.hdf5\"\n",
    "# Define that the accuracy in cv is monitored, and that weights are stored in a file when max accuracy is achieved\n",
    "checkpointA2 = ModelCheckpoint(filepathA2, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_listA2 = [checkpointA2]\n",
    "\n",
    "# Fit the model\n",
    "accs =[]\n",
    "val_accs =[]\n",
    "losss =[]\n",
    "val_losss =[]\n",
    "kappas = []\n",
    "aucs = []\n",
    "\n",
    "# Manually create epochs and reset between sessions\n",
    "for i in range(epochs):\n",
    "    # Single epoch. Remember to not shuffle the data!\n",
    "    print('Epoch', i+1, '/', epochs)\n",
    "    history = modelA2.fit(trainX, dummy_y_trainA, validation_data=(testX, dummy_y_testA), \n",
    "                        nb_epoch=1, batch_size=batch, shuffle=False, \n",
    "                        verbose=1, callbacks=callbacks_listA2)\n",
    "    modelA2.reset_states()\n",
    "    kappa, auc = printValStats(modelA2, testX, dummy_y_testA, batch=batch)\n",
    "    accs.append(history.history['acc'][0])\n",
    "    val_accs.append(history.history['val_acc'][0])\n",
    "    losss.append(history.history['loss'][0])\n",
    "    val_losss.append(history.history['val_loss'][0])\n",
    "    kappas.append(kappa)\n",
    "    aucs.append(auc)\n",
    "    \n",
    "print 'Best validation accuracy: ', get_max_values(val_accs)\n",
    "plot_training(accs, val_accs, losss, val_losss, kappas, aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-layer (wider) LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the model and parameters for training\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "batch = 1\n",
    "epochs = 100\n",
    "\n",
    "modelA1 = create_LSTM1_PCA(dummy_y_trainA.shape[1], batch_size = batch, trainShape1=n_components)\n",
    "print modelA1.summary()\n",
    "\n",
    "# To save the best model\n",
    "# serialize model to JSON\n",
    "modelA1_json = modelA1.to_json()\n",
    "with open(\"activity.model--1lstmbis.json\", \"w\") as json_file:\n",
    "    json_file.write(modelA1_json)\n",
    "filepathA1=\"activity.weights--1lstmbis.best.hdf5\"\n",
    "# Define that the accuracy in cv is monitored, and that weights are stored in a file when max accuracy is achieved\n",
    "checkpointA1 = ModelCheckpoint(filepathA1, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_listA1 = [checkpointA1]\n",
    "\n",
    "# Fit the model\n",
    "accs =[]\n",
    "val_accs =[]\n",
    "losss =[]\n",
    "val_losss =[]\n",
    "kappas = []\n",
    "aucs = []\n",
    "\n",
    "# Manually create epochs and reset between sessions\n",
    "for i in range(epochs):\n",
    "    # Single epoch. Remember to not shuffle the data!\n",
    "    print('Epoch', i+1, '/', epochs)\n",
    "    history = modelA1.fit(trainX, dummy_y_trainA, validation_data=(testX, dummy_y_testA), \n",
    "                        nb_epoch=1, batch_size=batch, shuffle=False, \n",
    "                        verbose=1, callbacks=callbacks_listA1)\n",
    "    modelA1.reset_states()\n",
    "    kappa, auc = printValStats(modelA1, testX, dummy_y_testA, batch=batch)\n",
    "    accs.append(history.history['acc'][0])\n",
    "    val_accs.append(history.history['val_acc'][0])\n",
    "    losss.append(history.history['loss'][0])\n",
    "    val_losss.append(history.history['val_loss'][0])\n",
    "    kappas.append(kappa)\n",
    "    aucs.append(auc)\n",
    "    \n",
    "print 'Best validation accuracy: ', get_max_values(val_accs)\n",
    "plot_training(accs, val_accs, losss, val_losss, kappas, aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and evaluating the best models\n",
    "\n",
    "## 3-layer LSTM (Activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 3 from disk\n",
      "3 Layer LSTM --- acc: 60.54%\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.605363984674\n",
      "Kappa:\n",
      "0.466174866304\n",
      "Confusion matrix:\n",
      "[[156   5  26  12   1]\n",
      " [ 10 241  56  11   7]\n",
      " [ 38  77 184  20  18]\n",
      " [ 25  12  26  30   2]\n",
      " [  6  45  11   4  21]]\n",
      "AUC score:\n",
      "0.776001092088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.46617486630418714, 0.77600109208809753)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "# LOAD AND USE MODEL\n",
    "json_file3 = open('activity.model--3lstmbis.json','r')\n",
    "loaded_model_json3 = json_file3.read()\n",
    "json_file3.close()\n",
    "loaded_model3 = model_from_json(loaded_model_json3)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model3.load_weights(\"activity.weights--3lstmbis.best.hdf5\")\n",
    "print(\"Loaded model 3 from disk\")\n",
    "# evaluate loaded model on test data\n",
    "# IMPORTANT: compile the model again before use!\n",
    "loaded_model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "score3 = loaded_model3.evaluate(testX, dummy_y_testA, batch_size=1, verbose=0)\n",
    "print \"3 Layer LSTM --- %s: %.2f%%\" % (loaded_model3.metrics_names[1], score3[1]*100)\n",
    "printValStats(loaded_model3,  testX, dummy_y_testA, batch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-layer LSTM (Activity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 2 from disk\n",
      "2 Layer LSTM --- acc: 62.74%\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.624521072797\n",
      "Kappa:\n",
      "0.487285752049\n",
      "Confusion matrix:\n",
      "[[161   5  25   9   0]\n",
      " [  3 269  42   0  11]\n",
      " [ 40  78 187  12  20]\n",
      " [ 31  17  26  18   3]\n",
      " [  4  61   5   0  17]]\n",
      "AUC score:\n",
      "0.808960195459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.48728575204930102, 0.80896019545938724)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD AND USE MODEL\n",
    "json_file2 = open('activity.model--2lstmbis.json','r')\n",
    "loaded_model_json2 = json_file2.read()\n",
    "json_file2.close()\n",
    "loaded_model2 = model_from_json(loaded_model_json2)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model2.load_weights(\"activity.weights--2lstmbis.best.hdf5\")\n",
    "print(\"Loaded model 2 from disk\")\n",
    "# evaluate loaded model on test data\n",
    "# IMPORTANT: compile the model again before use!\n",
    "loaded_model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "score2 = loaded_model2.evaluate(testX, dummy_y_testA, batch_size=1, verbose=0)\n",
    "print \"2 Layer LSTM --- %s: %.2f%%\" % (loaded_model2.metrics_names[1], score2[1]*100)\n",
    "printValStats(loaded_model2,  testX, dummy_y_testA, batch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-layer LSTM (Activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 1 from disk\n",
      "1 Layer LSTM --- acc: 62.74%\n",
      "Performance of model on test set ----------------------------\n",
      "Accuracy:\n",
      "0.621647509579\n",
      "Kappa:\n",
      "0.482466510546\n",
      "Confusion matrix:\n",
      "[[169   2  29   0   0]\n",
      " [ 13 250  53   0   9]\n",
      " [ 61  67 196   6   7]\n",
      " [ 41  11  27  16   0]\n",
      " [ 10  54   5   0  18]]\n",
      "AUC score:\n",
      "0.847622277625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4824665105456955, 0.84762227762486453)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD AND USE MODEL\n",
    "json_file1 = open('activity.model--1lstmbis.json','r')\n",
    "loaded_model_json1 = json_file1.read()\n",
    "json_file1.close()\n",
    "loaded_model1 = model_from_json(loaded_model_json1)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model1.load_weights(\"activity.weights--1lstmbis.best.hdf5\")\n",
    "print(\"Loaded model 1 from disk\")\n",
    "# evaluate loaded model on test data\n",
    "# IMPORTANT: compile the model again before use!\n",
    "loaded_model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "score1 = loaded_model1.evaluate(testX, dummy_y_testA, batch_size=1, verbose=0)\n",
    "print \"1 Layer LSTM --- %s: %.2f%%\" % (loaded_model1.metrics_names[1], score1[1]*100)\n",
    "printValStats(loaded_model1,  testX, dummy_y_testA, batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
